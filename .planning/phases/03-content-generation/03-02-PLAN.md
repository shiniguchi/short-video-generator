---
phase: 03-content-generation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/video_generator/__init__.py
  - app/services/video_generator/base.py
  - app/services/video_generator/mock.py
  - app/services/video_generator/svd.py
  - app/services/video_generator/chaining.py
  - app/services/video_generator/generator.py
  - app/services/voiceover_generator/__init__.py
  - app/services/voiceover_generator/base.py
  - app/services/voiceover_generator/mock.py
  - app/services/voiceover_generator/openai_tts.py
  - app/services/voiceover_generator/generator.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Mock video provider generates 9:16 vertical placeholder clips (720x1280) as MP4 files"
    - "Video clips can be chained to reach target duration (15-30 seconds)"
    - "Mock TTS provider generates silent audio files matching requested duration"
    - "Video and TTS provider backends are swappable via config setting without code changes"
    - "All providers work without GPU, Docker, or API keys (mock mode)"
  artifacts:
    - path: "app/services/video_generator/base.py"
      provides: "Abstract VideoProvider interface"
      contains: "class VideoProvider"
    - path: "app/services/video_generator/mock.py"
      provides: "MockVideoProvider generating placeholder clips"
      contains: "class MockVideoProvider"
    - path: "app/services/video_generator/chaining.py"
      provides: "Multi-clip concatenation to target duration"
      exports: ["chain_clips_to_duration"]
    - path: "app/services/video_generator/generator.py"
      provides: "VideoGeneratorService orchestrating provider + chaining"
      exports: ["VideoGeneratorService", "get_video_generator"]
    - path: "app/services/voiceover_generator/base.py"
      provides: "Abstract TTSProvider interface"
      contains: "class TTSProvider"
    - path: "app/services/voiceover_generator/mock.py"
      provides: "MockTTSProvider generating silent audio"
      contains: "class MockTTSProvider"
    - path: "app/services/voiceover_generator/openai_tts.py"
      provides: "OpenAI TTS provider implementation"
      contains: "class OpenAITTSProvider"
    - path: "app/services/voiceover_generator/generator.py"
      provides: "VoiceoverGeneratorService orchestrating provider"
      exports: ["VoiceoverGeneratorService", "get_voiceover_generator"]
  key_links:
    - from: "app/services/video_generator/generator.py"
      to: "app/services/video_generator/base.py"
      via: "Provider selection based on config"
      pattern: "video_provider_type"
    - from: "app/services/voiceover_generator/generator.py"
      to: "app/services/voiceover_generator/base.py"
      via: "Provider selection based on config"
      pattern: "tts_provider_type"
    - from: "app/services/video_generator/chaining.py"
      to: "moviepy"
      via: "concatenate_videoclips for clip assembly"
      pattern: "concatenate_videoclips"
---

<objective>
Build swappable video and voiceover generation backends with mock providers for local development.

Purpose: Implements the provider abstraction layer that allows video generation (SVD/Veo/mock) and TTS (OpenAI/ElevenLabs/mock) to be swapped via config. Mock providers enable full pipeline testing without GPU or API keys.

Output: Working MockVideoProvider (solid-color 9:16 clips), clip chaining logic, MockTTSProvider (silent audio), OpenAI TTS provider, and generator services that select provider based on config.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-content-generation/03-RESEARCH.md
@app/config.py
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Video provider abstraction with mock provider and clip chaining</name>
  <files>
    app/services/video_generator/__init__.py
    app/services/video_generator/base.py
    app/services/video_generator/mock.py
    app/services/video_generator/svd.py
    app/services/video_generator/chaining.py
    app/services/video_generator/generator.py
    requirements.txt
  </files>
  <action>
Create `app/services/video_generator/` package:

**base.py** - Abstract interface:
```python
from abc import ABC, abstractmethod
from typing import Optional

class VideoProvider(ABC):
    @abstractmethod
    def generate_clip(self, prompt: str, duration_seconds: int, width: int = 720, height: int = 1280) -> str:
        """Generate a video clip and return file path to MP4."""
        pass

    @abstractmethod
    def supports_resolution(self, width: int, height: int) -> bool:
        """Check if provider supports the given resolution."""
        pass
```

Note: Use synchronous interface (not async) because Celery tasks are synchronous. The established pattern uses `asyncio.run()` only for DB operations, not for generation.

**mock.py** - MockVideoProvider:
- `generate_clip()` creates a solid-color 9:16 vertical clip using moviepy's ColorClip
- Color varies per scene (use hash of prompt to pick from a palette of 6-8 colors)
- Writes to `{output_dir}/clips/mock_{uuid4()}.mp4` using moviepy's write_videofile
- fps=24, codec="libx264"
- Returns the file path
- `supports_resolution()` returns True for any resolution

**svd.py** - StableVideoDiffusionProvider (stub):
- `generate_clip()` raises `NotImplementedError("SVD requires GPU and Docker - use MockVideoProvider for local development")`
- Placeholder for future GPU deployment
- `supports_resolution()` returns True only for width<=1024, height<=1024 (SVD limitation)

**chaining.py** - Clip concatenation:
- `chain_clips_to_duration(clip_paths: List[str], target_duration: int, output_path: str) -> str`:
  - Load clips with `VideoFileClip(path)`
  - If total clip duration < target: repeat clips in sequence until >= target
  - Concatenate with `concatenate_videoclips(clips, method="compose")`
  - Trim to exact target duration with `subclip(0, target_duration)`
  - Write to output_path with fps=24, codec="libx264", audio_codec="aac"
  - Close all clips to free memory (important for moviepy)
  - Return output_path

**generator.py** - Orchestrator:
- `VideoGeneratorService`:
  - `__init__(self, provider: VideoProvider, output_dir: str)`
  - `generate_video(self, scenes: List[dict], target_duration: int, resolution: tuple = (720, 1280)) -> str`:
    - For each scene, call `provider.generate_clip(scene['visual_prompt'], scene['duration_seconds'], width, height)`
    - Collect clip paths
    - Call `chain_clips_to_duration(clip_paths, target_duration, output_path)`
    - Return final video path
- `get_video_generator() -> VideoGeneratorService`:
  - Factory function that reads `settings.video_provider_type`
  - "mock" -> MockVideoProvider, "svd" -> StableVideoDiffusionProvider
  - Creates output directory if not exists

**requirements.txt** - Add:
- `moviepy>=1.0.3` (video processing)
- `pillow>=10.0.0` (image generation for mock clips)
- `openai>=1.0.0` (for TTS in Task 2)

Use `from typing import List, Optional, Tuple` (Python 3.9). Use simple ABC pattern - NO dependency-injector library (per research: "simple ABC pattern, NO dependency-injector library"). Ensure all file operations create directories with `os.makedirs(exist_ok=True)`.
  </action>
  <verify>
```python
import os
os.makedirs("output/clips", exist_ok=True)
from app.services.video_generator.generator import get_video_generator
gen = get_video_generator()
# Generate a single mock clip
scenes = [{"visual_prompt": "A serene lake at sunset", "duration_seconds": 3}]
video_path = gen.generate_video(scenes, target_duration=3)
assert os.path.exists(video_path)
assert video_path.endswith(".mp4")
print(f"Generated video: {video_path}")

# Test chaining
scenes_multi = [
    {"visual_prompt": "Mountain landscape", "duration_seconds": 3},
    {"visual_prompt": "City skyline at night", "duration_seconds": 3},
]
video_path2 = gen.generate_video(scenes_multi, target_duration=6)
assert os.path.exists(video_path2)
print(f"Chained video: {video_path2}")
```
  </verify>
  <done>MockVideoProvider generates 720x1280 solid-color MP4 clips. chain_clips_to_duration concatenates clips to target duration. get_video_generator() returns service configured by VIDEO_PROVIDER_TYPE setting. All files created in output/clips/ directory.</done>
</task>

<task type="auto">
  <name>Task 2: TTS provider abstraction with mock and OpenAI providers</name>
  <files>
    app/services/voiceover_generator/__init__.py
    app/services/voiceover_generator/base.py
    app/services/voiceover_generator/mock.py
    app/services/voiceover_generator/openai_tts.py
    app/services/voiceover_generator/generator.py
  </files>
  <action>
Create `app/services/voiceover_generator/` package:

**base.py** - Abstract interface:
```python
from abc import ABC, abstractmethod

class TTSProvider(ABC):
    @abstractmethod
    def generate_speech(self, text: str, voice: str = "default", output_path: str = None) -> str:
        """Generate speech audio from text, return file path."""
        pass

    @abstractmethod
    def get_available_voices(self) -> List[str]:
        """List available voice options."""
        pass
```

Synchronous interface (same rationale as video provider - Celery tasks are sync).

**mock.py** - MockTTSProvider:
- `generate_speech()` generates a silent WAV file:
  - Calculate duration based on text length: `duration = max(1.0, len(text) / 15.0)` (~15 chars per second)
  - Use `numpy` to create silent audio array (zeros) at 44100 Hz sample rate
  - Write as WAV using `scipy.io.wavfile.write()` OR use moviepy's AudioClip:
    ```python
    import numpy as np
    from moviepy.audio.AudioClip import AudioClip
    def make_frame(t):
        return np.array([[0.0, 0.0]])  # Stereo silence
    audio = AudioClip(make_frame, duration=duration, fps=44100)
    audio.write_audiofile(output_path, logger=None)
    audio.close()
    ```
  - Save to `{output_dir}/audio/mock_{uuid4()}.mp3`
  - Return file path
- `get_available_voices()` returns ["mock"]

**openai_tts.py** - OpenAITTSProvider:
- `__init__(self, api_key: str)` - stores API key, creates OpenAI client lazily
- `generate_speech()`:
  - If `settings.use_mock_data` is True or api_key is empty, delegate to MockTTSProvider (safety net)
  - Otherwise call OpenAI TTS API:
    ```python
    from openai import OpenAI
    client = OpenAI(api_key=self.api_key)
    response = client.audio.speech.create(
        model="tts-1-hd",
        voice=voice,  # alloy, echo, fable, onyx, nova, shimmer
        input=text,
        response_format="mp3"
    )
    response.stream_to_file(output_path)
    ```
  - Save to `{output_dir}/audio/{uuid4()}.mp3`
  - Return file path
- `get_available_voices()` returns ["alloy", "echo", "fable", "onyx", "nova", "shimmer"]

**generator.py** - Orchestrator:
- `VoiceoverGeneratorService`:
  - `__init__(self, provider: TTSProvider, output_dir: str)`
  - `generate_voiceover(self, script: str, voice: str = "default", target_duration: Optional[float] = None) -> str`:
    - Call `provider.generate_speech(script, voice, output_path)`
    - If `target_duration` provided, check audio duration using moviepy's AudioFileClip:
      - If audio too long: trim with `subclip(0, target_duration)`
      - If audio too short: pad with silence to match (rare for TTS)
    - Return audio file path
- `get_voiceover_generator() -> VoiceoverGeneratorService`:
  - Factory function reads `settings.tts_provider_type`
  - "mock" -> MockTTSProvider, "openai" -> OpenAITTSProvider(api_key=settings.openai_api_key)
  - Creates output directory if not exists

Use `from typing import List, Optional` (Python 3.9). Ensure `os.makedirs(output_dir + "/audio", exist_ok=True)` in factory.
  </action>
  <verify>
```python
import os
os.makedirs("output/audio", exist_ok=True)
from app.services.voiceover_generator.generator import get_voiceover_generator
gen = get_voiceover_generator()
# Generate mock voiceover
audio_path = gen.generate_voiceover("Hello world, this is a test voiceover script for our video.", voice="default")
assert os.path.exists(audio_path)
print(f"Generated audio: {audio_path}")

# Verify it has reasonable duration
from moviepy.editor import AudioFileClip
clip = AudioFileClip(audio_path)
print(f"Audio duration: {clip.duration:.1f}s")
assert clip.duration > 0
clip.close()
```
  </verify>
  <done>MockTTSProvider generates silent audio files with duration proportional to text length. OpenAITTSProvider falls back to mock when USE_MOCK_DATA=true or no API key. get_voiceover_generator() returns service configured by TTS_PROVIDER_TYPE setting. Audio files created in output/audio/ directory.</done>
</task>

</tasks>

<verification>
1. `python -c "from app.services.video_generator.generator import get_video_generator; print('Video OK')"` imports clean
2. `python -c "from app.services.voiceover_generator.generator import get_voiceover_generator; print('Voice OK')"` imports clean
3. Mock video generates MP4 file at 720x1280 resolution
4. Mock TTS generates audio file with duration > 0
5. Both providers selected via config settings (no hardcoded provider)
6. No GPU, Docker, or API keys required for mock mode
</verification>

<success_criteria>
- MockVideoProvider creates 9:16 vertical MP4 clips with solid colors
- Clip chaining concatenates multiple clips to target duration (15-30s)
- MockTTSProvider creates silent audio with duration based on text length
- OpenAITTSProvider has working implementation (falls back to mock when USE_MOCK_DATA=true)
- Provider selection via VIDEO_PROVIDER_TYPE and TTS_PROVIDER_TYPE settings
- SVD provider stub exists for future GPU deployment
- All file output goes to output/ directory
- moviepy and openai added to requirements.txt
- All code Python 3.9 compatible
</success_criteria>

<output>
After completion, create `.planning/phases/03-content-generation/03-02-SUMMARY.md`
</output>
