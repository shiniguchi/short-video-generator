---
phase: 06-pipeline-integration
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - app/api/routes.py
  - app/schemas.py
autonomous: true

must_haves:
  truths:
    - "POST /api/generate creates a Job record and triggers the pipeline without blocking the response"
    - "GET /api/jobs/{id} returns real-time pipeline status including current stage, completed stages, and error details"
    - "GET /api/jobs lists all pipeline jobs with status and stage info"
    - "POST /api/jobs/{id}/retry resets a failed job and re-triggers the pipeline from last checkpoint"
    - "Pipeline status is visible via REST API in real-time"
  artifacts:
    - path: "app/api/routes.py"
      provides: "Pipeline trigger, status, list, and retry endpoints"
      contains: "/generate"
    - path: "app/schemas.py"
      provides: "Pydantic schemas for pipeline request/response"
      contains: "PipelineTriggerRequest"
  key_links:
    - from: "app/api/routes.py"
      to: "app/pipeline.py"
      via: "imports orchestrate_pipeline_task for triggering"
      pattern: "from app\\.pipeline import"
    - from: "app/api/routes.py"
      to: "app/models.py"
      via: "queries Job model for status"
      pattern: "from app\\.models import Job"
    - from: "app/api/routes.py"
      to: "app/pipeline.py"
      via: "imports PIPELINE_STAGES for response context"
      pattern: "PIPELINE_STAGES"
---

<objective>
Add REST API endpoints for triggering, monitoring, listing, and retrying pipeline runs. This completes ORCH-04 and ORCH-05, making the pipeline controllable and observable via HTTP.

Purpose: Users need to trigger pipeline runs and monitor their progress without accessing Celery internals or logs directly.
Output: Four new endpoints in `app/api/routes.py` and supporting Pydantic schemas in `app/schemas.py`.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-pipeline-integration/06-RESEARCH.md
@.planning/phases/06-pipeline-integration/06-01-SUMMARY.md
@app/models.py
@app/api/routes.py
@app/schemas.py
@app/pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Pydantic schemas for pipeline endpoints</name>
  <files>app/schemas.py</files>
  <action>
Add the following schemas to the END of `app/schemas.py` (after the existing VideoProductionPlanResponse class):

```python
# Pipeline Orchestration Schemas (Phase 6)

class PipelineTriggerRequest(BaseModel):
    """Request body for POST /api/generate."""
    theme: Optional[str] = None  # Theme override (uses sample-data.yml default if None)
    config_path: Optional[str] = None  # Custom config file path

class PipelineTriggerResponse(BaseModel):
    """Response for POST /api/generate."""
    job_id: int
    task_id: str
    status: str  # "queued"
    poll_url: str  # "/api/jobs/{job_id}"
    message: str

class JobStatusResponse(BaseModel):
    """Response for GET /api/jobs/{id}."""
    id: int
    status: str  # pending, running, completed, failed
    stage: Optional[str]  # Current or last pipeline stage
    theme: Optional[str]
    created_at: Optional[str]  # ISO format
    updated_at: Optional[str]  # ISO format
    error_message: Optional[str]
    completed_stages: Optional[List[str]]  # Stages that finished successfully
    total_stages: int  # Total pipeline stages count
    progress_pct: Optional[float]  # Percentage complete (completed/total * 100)

class JobListResponse(BaseModel):
    """Response for GET /api/jobs."""
    count: int
    jobs: List[JobStatusResponse]

class JobRetryResponse(BaseModel):
    """Response for POST /api/jobs/{id}/retry."""
    job_id: int
    task_id: str
    status: str  # "queued"
    resume_from: Optional[str]  # Stage resuming from
    skipping_stages: List[str]  # Already completed stages
    message: str
```

Use `from typing import List, Optional` (already imported at top of file). Do NOT use Python 3.10+ `list[str]` syntax.
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && source venv/bin/activate && python -c "from app.schemas import PipelineTriggerRequest, PipelineTriggerResponse, JobStatusResponse, JobListResponse, JobRetryResponse; print('All pipeline schemas importable')"`
  </verify>
  <done>
  - PipelineTriggerRequest, PipelineTriggerResponse, JobStatusResponse, JobListResponse, JobRetryResponse schemas defined
  - All use Python 3.9 compatible typing imports
  - Schemas validate correctly via Pydantic
  </done>
</task>

<task type="auto">
  <name>Task 2: Add pipeline REST API endpoints</name>
  <files>app/api/routes.py</files>
  <action>
Add a new section at the END of `app/api/routes.py` (after the Phase 5 endpoints):

```python
# --- Phase 6: Pipeline Integration ---
```

Add four endpoints:

**1. POST /generate** (ORCH-05 - Manual pipeline trigger):
- Accepts optional JSON body via `PipelineTriggerRequest` (use `request: PipelineTriggerRequest = PipelineTriggerRequest()` as default for empty body support)
- Depends on `session: AsyncSession = Depends(get_session)`
- Creates a new `Job` record with: `status="pending"`, `stage="initialization"`, `theme=request.theme or "default"`, `extra_data={"completed_stages": [], "config_path": request.config_path}`
- `session.add(job)`, `await session.commit()`, `await session.refresh(job)`
- Imports `orchestrate_pipeline_task` from `app.pipeline` (lazy import inside function body)
- Triggers task: `task = orchestrate_pipeline_task.delay(job_id=job.id, theme_config_path=request.config_path)`
- Returns `PipelineTriggerResponse` with job_id, task_id=str(task.id), status="queued", poll_url=f"/api/jobs/{job.id}", message="Pipeline execution started"
- CRITICAL: Do NOT await the task result. Just trigger and return immediately (non-blocking).

**2. GET /jobs** (ORCH-04 - List pipeline jobs):
- Accepts optional query params: `status: Optional[str] = None`, `limit: int = 20`
- Depends on `session: AsyncSession = Depends(get_session)`
- Queries `Job` model ordered by `created_at.desc()`, filtered by status if provided, limited
- For each job, compute: `completed_stages = job.extra_data.get("completed_stages", []) if job.extra_data else []`
- Import `PIPELINE_STAGES` from `app.pipeline` at function level
- Compute `total_stages = len(PIPELINE_STAGES)` and `progress_pct = round(len(completed_stages) / total_stages * 100, 1) if total_stages > 0 else 0`
- Return `JobListResponse` with count and list of `JobStatusResponse` dicts

**3. GET /jobs/{job_id}** (ORCH-04 - Get single job status):
- Depends on `session: AsyncSession = Depends(get_session)`
- Queries `Job` by id, raises `HTTPException(404)` if not found
- Computes `completed_stages` and `progress_pct` same as above
- Returns `JobStatusResponse` dict with all fields including completed_stages, total_stages, progress_pct
- Format datetime fields as `.isoformat()` (handle None with conditional)

**4. POST /jobs/{job_id}/retry** (Resume failed pipeline):
- Depends on `session: AsyncSession = Depends(get_session)`
- Queries `Job` by id, raises `HTTPException(404)` if not found
- Validates: if `job.status != "failed"`, raise `HTTPException(400, detail=f"Cannot retry job with status '{job.status}'. Only 'failed' jobs can be retried.")`
- Resets job: `job.status = "pending"`, `job.error_message = None`, `job.updated_at = datetime.now(timezone.utc)`
- Keep `extra_data["completed_stages"]` intact (this is how resume works)
- `await session.commit()`
- Imports `orchestrate_pipeline_task` from `app.pipeline`
- Triggers: `task = orchestrate_pipeline_task.delay(job_id=job.id, theme_config_path=job.extra_data.get("config_path") if job.extra_data else None, resume=True)`
- Compute `completed_stages` and `resume_from` (first stage NOT in completed_stages)
- Returns `JobRetryResponse` with job_id, task_id, status="queued", resume_from, skipping_stages=completed_stages, message

Add necessary imports at top of routes.py:
- `from app.schemas import PipelineTriggerRequest` (add to imports section, or import lazily inside functions -- lazy is safer to avoid circular imports)
- `from app.models import Job` can also be lazy-imported inside each function (follow existing pattern in the file)

Python 3.9 compatibility: Use `from typing import Optional` (already imported). Use `datetime.now(timezone.utc)` (already imported).
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && source venv/bin/activate && python -c "
from app.main import app
from fastapi.testclient import TestClient
routes = [r.path for r in app.routes]
assert '/api/generate' in routes, f'/api/generate not found in {routes}'
assert '/api/jobs' in routes or '/api/jobs/' in routes, f'/api/jobs not found in {routes}'
assert '/api/jobs/{job_id}' in routes, f'/api/jobs/id not found in {routes}'
assert '/api/jobs/{job_id}/retry' in routes, f'/api/jobs/id/retry not found in {routes}'
print('All pipeline endpoints registered')
"`
  </verify>
  <done>
  - POST /api/generate creates Job and triggers pipeline asynchronously
  - GET /api/jobs lists all pipeline jobs with status, stage, progress percentage
  - GET /api/jobs/{id} returns detailed real-time status for a single job
  - POST /api/jobs/{id}/retry resets failed job and re-triggers from last checkpoint
  - All endpoints are non-blocking (trigger task.delay, don't wait for result)
  - Progress percentage computed from completed_stages / total_stages
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.schemas import PipelineTriggerRequest, JobStatusResponse, JobRetryResponse; print('Schemas OK')"` succeeds
2. All 4 new endpoints registered in FastAPI app routes
3. Existing endpoints (health, trends, videos, approve, reject) still functional
4. `python -c "from app.api.routes import router; print([r.path for r in router.routes])"` shows all endpoints including new Phase 6 ones
</verification>

<success_criteria>
- POST /api/generate triggers full pipeline and returns job_id immediately
- GET /api/jobs/{id} returns stage, status, completed_stages, progress_pct in real-time
- GET /api/jobs lists pipeline jobs with pagination and optional status filter
- POST /api/jobs/{id}/retry resumes failed pipeline from last checkpoint
- All responses use Pydantic schemas for consistent structure
- No blocking operations in async endpoint handlers
- Python 3.9 compatible
</success_criteria>

<output>
After completion, create `.planning/phases/06-pipeline-integration/06-02-SUMMARY.md`
</output>
