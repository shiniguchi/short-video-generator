---
phase: 06-pipeline-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/pipeline.py
  - app/worker.py
autonomous: true

must_haves:
  truths:
    - "Pipeline executes all 5 orchestration stages in sequence: trend collection, trend analysis, content generation (encompasses script + video + voiceover), composition, review"
    - "Each stage completion is persisted to the Job record in the database before advancing"
    - "Failed pipeline can resume from last completed stage without re-running earlier stages"
    - "Failed stages retry up to configurable limit with exponential backoff"
    - "Pipeline logs each stage transition to Python logger for Docker log visibility"
  artifacts:
    - path: "app/pipeline.py"
      provides: "Stage constants, orchestrate_pipeline_task, job status helpers"
      contains: "PIPELINE_STAGES"
    - path: "app/worker.py"
      provides: "Celery autodiscovery registration for pipeline module"
      contains: "app.pipeline"
  key_links:
    - from: "app/pipeline.py"
      to: "app/tasks.py"
      via: "imports collect_trends_task, analyze_trends_task, generate_content_task"
      pattern: "from app\\.tasks import"
    - from: "app/pipeline.py"
      to: "app/database.py"
      via: "async_session_factory for Job status updates"
      pattern: "async_session_factory"
    - from: "app/pipeline.py"
      to: "app/models.py"
      via: "Job model for stage tracking"
      pattern: "from app\\.models import Job"
---

<objective>
Build the pipeline orchestration task that sequences all existing stage tasks with database-backed checkpointing, resume-from-checkpoint capability, and per-stage retry logic.

Purpose: This is the core orchestration layer (ORCH-01, ORCH-02, ORCH-03) that transforms independent Celery tasks into a cohesive pipeline with fault tolerance.
Output: `app/pipeline.py` with `orchestrate_pipeline_task`, stage constants, and Job status helpers.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-pipeline-integration/06-RESEARCH.md
@app/models.py
@app/tasks.py
@app/database.py
@app/worker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pipeline orchestration module with stage constants and Job helpers</name>
  <files>app/pipeline.py</files>
  <action>
Create `app/pipeline.py` with the following components:

1. **Stage Constants** (single source of truth for stage names):
```python
STAGE_TREND_COLLECTION = "trend_collection"
STAGE_TREND_ANALYSIS = "trend_analysis"
STAGE_CONTENT_GENERATION = "content_generation"
STAGE_COMPOSITION = "composition"
STAGE_REVIEW = "review"

PIPELINE_STAGES = [
    STAGE_TREND_COLLECTION,
    STAGE_TREND_ANALYSIS,
    STAGE_CONTENT_GENERATION,
    STAGE_COMPOSITION,
    STAGE_REVIEW,
]
```

2. **Async Job helper functions** (called via `asyncio.run()` from sync Celery context):
   - `_load_job(job_id)` -> returns dict with id, status, stage, theme, extra_data, error_message
   - `_update_job_status(job_id, stage, status, error_msg=None)` -> updates Job row
   - `_mark_stage_complete(job_id, stage)` -> appends stage to `extra_data["completed_stages"]` list, updates status to "running"
   - `_mark_job_complete(job_id)` -> sets status="completed", stage to last stage
   - `_mark_job_failed(job_id, stage, error_msg)` -> sets status="failed", stores error_message

   All helpers use `from app.database import async_session_factory` and `from app.models import Job`. Use `from sqlalchemy import select`. Import `from datetime import datetime, timezone` for `updated_at`. Handle the SQLAlchemy `extra_data` column (mapped to "metadata" in DB) carefully - when updating JSON, reassign the dict to trigger SQLAlchemy dirty detection: `job.extra_data = {**existing, "completed_stages": updated_list}`.

3. **`orchestrate_pipeline_task`** Celery task:
   - Decorator: `@celery_app.task(bind=True, name='app.tasks.orchestrate_pipeline_task', max_retries=0)` (orchestrator itself does NOT retry - individual stages handle their own retries via existing autoretry_for config)
   - Signature: `def orchestrate_pipeline_task(self, job_id: int, theme_config_path: Optional[str] = None, resume: bool = False)`
   - Import `celery_app` from `app.worker`
   - Import stage tasks lazily inside the function body to avoid circular imports: `from app.tasks import collect_trends_task, analyze_trends_task, generate_content_task`
   - NOTE: Do NOT import compose_video_task - it is automatically chained by generate_content_task

   **Pipeline logic:**
   a. Load job via `asyncio.run(_load_job(job_id))`
   b. Get `completed_stages` from `job["extra_data"].get("completed_stages", [])` if `job["extra_data"]` else `[]`
   c. Update job status to "running" via `asyncio.run(_update_job_status(job_id, PIPELINE_STAGES[0], "running"))`
   d. Define stage-to-task mapping (list of tuples):
      ```python
      stage_tasks = [
          (STAGE_TREND_COLLECTION, collect_trends_task, []),
          (STAGE_TREND_ANALYSIS, analyze_trends_task, []),
          (STAGE_CONTENT_GENERATION, generate_content_task, [theme_config_path]),
      ]
      ```
   e. Loop through `stage_tasks`:
      - If `stage_name in completed_stages`: log skip, continue
      - Log "Starting stage: {stage_name}"
      - `asyncio.run(_update_job_status(job_id, stage_name, "running"))`
      - Execute task synchronously: `result = task_func.apply_async(args=args)` then `task_result = result.get(timeout=1800)` (30 min timeout)
      - Log "Stage {stage_name} completed: {task_result}"
      - `asyncio.run(_mark_stage_complete(job_id, stage_name))`
      - For STAGE_CONTENT_GENERATION: after completion, extract `compose_task_id` from task_result. If present, wait for composition: `compose_result = celery_app.AsyncResult(compose_task_id); compose_result.get(timeout=1800)`. Then mark STAGE_COMPOSITION complete too: `asyncio.run(_mark_stage_complete(job_id, STAGE_COMPOSITION))`
   f. After all stages: `asyncio.run(_mark_stage_complete(job_id, STAGE_REVIEW))` then `asyncio.run(_mark_job_complete(job_id))`
   g. Return `{"status": "completed", "job_id": job_id, "completed_stages": PIPELINE_STAGES}`

   **Error handling:**
   - Wrap the entire stage loop in try/except
   - On Exception: log error with stage name, call `asyncio.run(_mark_job_failed(job_id, stage_name, str(exc)))`, then re-raise
   - The individual stage tasks already have `autoretry_for=(Exception,)` with `retry_backoff=True` and `max_retries=3`, so transient failures will be retried at the task level before bubbling up to the orchestrator

   **Python 3.9 compatibility:** Use `from typing import Optional, List, Dict` not `list[str]` syntax. Use `asyncio.run()` pattern consistent with existing tasks.

   **Logging:** Use `logger = logging.getLogger(__name__)` with `logger.info()` for stage transitions, `logger.error()` for failures. These will appear in Docker container logs (ORCH-04).
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && source venv/bin/activate && python -c "from app.pipeline import orchestrate_pipeline_task, PIPELINE_STAGES; print('Stages:', PIPELINE_STAGES); print('Task name:', orchestrate_pipeline_task.name)"`

Expected output should show all 5 stages and task name 'app.tasks.orchestrate_pipeline_task'.
  </verify>
  <done>
  - orchestrate_pipeline_task is a registered Celery task that sequences collect_trends -> analyze_trends -> generate_content (with composition chaining) -> review
  - Stage constants defined as module-level variables in PIPELINE_STAGES list
  - Job status helpers update the Job model after every stage transition
  - completed_stages tracked in Job.extra_data for resume capability
  - Resume mode skips already-completed stages
  - Individual stage retries handled by existing task-level autoretry_for config
  </done>
</task>

<task type="auto">
  <name>Task 2: Register pipeline module with Celery autodiscovery</name>
  <files>app/worker.py</files>
  <action>
Update `celery_app.autodiscover_tasks(['app'])` in `app/worker.py` to also discover the pipeline module. Change to:

```python
celery_app.autodiscover_tasks(['app'], related=['pipeline'])
```

Or alternatively, since autodiscover_tasks with the 'app' package should find `app.pipeline` automatically if the task is decorated with `@celery_app.task`, verify this works. If the autodiscovery does NOT pick up pipeline.py (because it only scans tasks.py by default), add an explicit import in worker.py:

```python
# Ensure pipeline tasks are registered
import app.pipeline  # noqa: F401
```

Place this import AFTER the celery_app configuration block, before the autodiscover line.

Additionally, add `'app.pipeline'` to the autodiscover list: `celery_app.autodiscover_tasks(['app', 'app.pipeline'])` -- but this may not be needed. The safest approach is the explicit import.
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && source venv/bin/activate && python -c "from app.worker import celery_app; print([t for t in celery_app.tasks if 'orchestrate' in t or 'pipeline' in t])"`

Expected: Should list 'app.tasks.orchestrate_pipeline_task'.
  </verify>
  <done>
  - Celery worker discovers and registers orchestrate_pipeline_task
  - Task is callable via celery_app with name 'app.tasks.orchestrate_pipeline_task'
  </done>
</task>

</tasks>

<verification>
1. `python -c "from app.pipeline import PIPELINE_STAGES, orchestrate_pipeline_task; print(PIPELINE_STAGES)"` prints 5 stages
2. `python -c "from app.pipeline import _load_job, _update_job_status, _mark_stage_complete, _mark_job_complete, _mark_job_failed; print('All helpers importable')"` succeeds
3. `python -c "from app.worker import celery_app; print('orchestrate' in str(celery_app.tasks))"` returns True
4. All existing tasks still importable: `python -c "from app.tasks import collect_trends_task, analyze_trends_task, generate_content_task, compose_video_task; print('All tasks OK')"`
</verification>

<success_criteria>
- orchestrate_pipeline_task registered and importable as a Celery task
- PIPELINE_STAGES contains 5 stages in correct order
- Job helpers update database correctly (stage, status, completed_stages, error_message)
- Resume capability: completed_stages list in extra_data enables skipping finished stages
- No modifications to existing stage tasks (they retain their own retry logic)
- Python 3.9 compatible (typing imports, no 3.10+ syntax)
</success_criteria>

<output>
After completion, create `.planning/phases/06-pipeline-integration/06-01-SUMMARY.md`
</output>
