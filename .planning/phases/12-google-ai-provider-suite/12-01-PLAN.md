---
phase: 12-google-ai-provider-suite
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/services/llm_provider/base.py
  - app/services/llm_provider/mock.py
  - app/services/llm_provider/gemini.py
  - app/services/llm_provider/__init__.py
  - app/config.py
autonomous: true

must_haves:
  truths:
    - "LLMProvider abstraction generates structured text matching any Pydantic schema"
    - "GeminiLLMProvider uses google-generativeai SDK with native JSON mode for structured output"
    - "MockLLMProvider returns deterministic mock data for any schema without API calls"
    - "Factory function selects provider based on LLM_PROVIDER_TYPE setting"
    - "Provider falls back to mock when GOOGLE_API_KEY is missing or USE_MOCK_DATA=true"
  artifacts:
    - path: "app/services/llm_provider/base.py"
      provides: "LLMProvider ABC with generate_structured() and generate_text()"
      contains: "class LLMProvider"
    - path: "app/services/llm_provider/gemini.py"
      provides: "Gemini implementation using google-generativeai SDK"
      contains: "class GeminiLLMProvider"
    - path: "app/services/llm_provider/mock.py"
      provides: "Mock LLM provider for testing"
      contains: "class MockLLMProvider"
    - path: "app/services/llm_provider/__init__.py"
      provides: "Factory function get_llm_provider()"
      contains: "def get_llm_provider"
    - path: "app/config.py"
      provides: "google_api_key and llm_provider_type settings"
      contains: "google_api_key"
  key_links:
    - from: "app/services/llm_provider/__init__.py"
      to: "app/config.py"
      via: "get_settings() for provider selection"
      pattern: "get_settings\\(\\)"
    - from: "app/services/llm_provider/gemini.py"
      to: "google.generativeai"
      via: "SDK import for API calls"
      pattern: "import google.generativeai"
---

<objective>
Create LLM Provider abstraction layer with Gemini implementation.

Purpose: Establish a provider-agnostic LLM interface so script_generator and trend_analyzer can use Gemini (or Claude, or mock) without direct SDK coupling. This is the foundation for replacing Anthropic-specific code with config-driven LLM selection.

Output: New `app/services/llm_provider/` package with ABC, mock, Gemini providers, factory function, and config settings.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-google-ai-provider-suite/12-RESEARCH.md
@app/config.py
@app/services/video_generator/base.py
@app/services/video_generator/mock.py
@app/services/voiceover_generator/base.py
@app/services/voiceover_generator/mock.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LLM Provider ABC, Mock, and Gemini implementations</name>
  <files>
    app/services/llm_provider/base.py
    app/services/llm_provider/mock.py
    app/services/llm_provider/gemini.py
  </files>
  <action>
Create `app/services/llm_provider/` directory with three files:

**base.py** - LLMProvider ABC with two abstract methods:
- `generate_structured(prompt: str, schema: Type[BaseModel], system_prompt: Optional[str] = None, temperature: float = 1.0) -> BaseModel` - Generate structured output matching Pydantic schema
- `generate_text(prompt: str, system_prompt: Optional[str] = None, temperature: float = 1.0, max_tokens: int = 4096) -> str` - Generate freeform text output
- Use `from typing import Type, Optional` (Python 3.9 compatibility)
- Follow exact same ABC pattern as `app/services/video_generator/base.py`

**mock.py** - MockLLMProvider:
- Constructor takes no required args (mock needs no config)
- `generate_structured()`: Create an instance of the schema with default/empty values. Use `schema.model_json_schema()` to inspect fields, then build a dict with sensible defaults (empty strings for str, 0 for int/float, empty lists for List, False for bool). Validate with `schema(**defaults)` and return. Log "MockLLMProvider: returning mock structured output for {schema.__name__}"
- `generate_text()`: Return a mock string like "Mock LLM response for: {prompt[:100]}..." Log the call.
- Follow pattern from `app/services/voiceover_generator/mock.py` (constructor, logger, etc.)

**gemini.py** - GeminiLLMProvider:
- Constructor takes `api_key: str`. Calls `google.generativeai.configure(api_key=api_key)`. Stores model name as `self.model_name = "gemini-2.5-flash"`.
- `generate_structured()`: Use Gemini's native JSON mode:
  - Create `GenerativeModel(self.model_name)` (fresh instance each call to avoid state issues)
  - Set `generation_config = {"temperature": temperature, "response_mime_type": "application/json", "response_json_schema": schema.model_json_schema()}` (NOTE: the deprecated google-generativeai SDK 0.8.x uses `response_json_schema` with a dict from `model_json_schema()`, NOT `response_schema` with Pydantic class -- per research Pattern 2)
  - Build full_prompt by prepending system_prompt if provided
  - Call `model.generate_content(full_prompt, generation_config=generation_config)`
  - Parse response: `schema.model_validate_json(response.text)`
  - Wrap in retry using tenacity: `@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=2, min=4, max=30))` on the actual API call
- `generate_text()`: Standard text generation without JSON mode:
  - `generation_config = {"temperature": temperature, "max_output_tokens": max_tokens}`
  - Same model creation, prompt building, content generation
  - Return `response.text`
  - Same retry decorator
- Import pattern: `import google.generativeai as genai` then `genai.configure(...)`, `genai.GenerativeModel(...)`
- Handle errors: catch `google.api_core.exceptions.ResourceExhausted` for rate limiting (logged but re-raised for tenacity)
  </action>
  <verify>
Run `python -c "from app.services.llm_provider.base import LLMProvider; from app.services.llm_provider.mock import MockLLMProvider; from app.services.llm_provider.gemini import GeminiLLMProvider; print('All LLM providers import successfully')"` to confirm imports work.

Then test mock provider:
```python
python -c "
from app.services.llm_provider.mock import MockLLMProvider
from pydantic import BaseModel
from typing import List

class TestSchema(BaseModel):
    title: str
    count: int
    items: List[str]

mock = MockLLMProvider()
result = mock.generate_structured('test prompt', TestSchema)
print(f'Structured: {result}')
text = mock.generate_text('test prompt')
print(f'Text: {text}')
print('Mock LLM provider works')
"
```
  </verify>
  <done>LLMProvider ABC exists with generate_structured() and generate_text() methods. MockLLMProvider generates valid mock data for any Pydantic schema. GeminiLLMProvider uses google-generativeai SDK with native JSON mode and tenacity retry.</done>
</task>

<task type="auto">
  <name>Task 2: Create factory function and update config with Google API settings</name>
  <files>
    app/services/llm_provider/__init__.py
    app/config.py
  </files>
  <action>
**__init__.py** - Factory function + exports:
- `get_llm_provider() -> LLMProvider` factory function:
  - Gets settings via `get_settings()`
  - Reads `settings.llm_provider_type` (default "mock")
  - If `"gemini"`: check `settings.google_api_key` -- if empty or `settings.use_mock_data` is True, log warning and return MockLLMProvider(). Otherwise return `GeminiLLMProvider(api_key=settings.google_api_key)`
  - Else (including "mock"): return `MockLLMProvider()`
  - Follow exact same factory pattern as `app/services/video_generator/generator.py::get_video_generator()`
- Export: `LLMProvider`, `MockLLMProvider`, `GeminiLLMProvider`, `get_llm_provider`

**app/config.py** - Add to Settings class (after the `heygen_avatar_id` line, before `output_dir`):
- `google_api_key: str = ""  # Google AI API key (Gemini, Imagen, Veo)`
- `llm_provider_type: str = "mock"  # mock/gemini`
- `image_provider_type: str = "mock"  # mock/imagen`
- Add all three config fields now (image_provider_type will be used by Plan 02, adding it now avoids config.py file conflict between parallel plans)
- Keep all existing fields unchanged
  </action>
  <verify>
Run:
```python
python -c "
from app.config import get_settings
s = get_settings()
print(f'google_api_key: \"{s.google_api_key}\"')
print(f'llm_provider_type: \"{s.llm_provider_type}\"')
print(f'image_provider_type: \"{s.image_provider_type}\"')

from app.services.llm_provider import get_llm_provider, LLMProvider, MockLLMProvider
provider = get_llm_provider()
print(f'Provider type: {type(provider).__name__}')
assert isinstance(provider, MockLLMProvider), 'Should be mock by default'
print('Factory returns MockLLMProvider by default - OK')
"
```
  </verify>
  <done>Factory function get_llm_provider() returns correct provider based on LLM_PROVIDER_TYPE setting. Config has google_api_key, llm_provider_type, and image_provider_type fields. Default is mock provider.</done>
</task>

</tasks>

<verification>
1. `python -c "from app.services.llm_provider import get_llm_provider; p = get_llm_provider(); print(type(p).__name__)"` prints "MockLLMProvider"
2. LLMProvider ABC has both `generate_structured` and `generate_text` abstract methods
3. MockLLMProvider generates valid Pydantic model instances
4. GeminiLLMProvider imports without error (doesn't need API key to import)
5. Config has google_api_key, llm_provider_type, image_provider_type fields
</verification>

<success_criteria>
- LLMProvider abstraction is importable and functional with mock provider
- GeminiLLMProvider uses google-generativeai SDK (deprecated but Python 3.9 compatible)
- Factory function correctly selects provider based on config
- Config settings added for Google API key and provider type selection
- All Python 3.9 typing patterns used (from typing import List, Optional, Type)
</success_criteria>

<output>
After completion, create `.planning/phases/12-google-ai-provider-suite/12-01-SUMMARY.md`
</output>
