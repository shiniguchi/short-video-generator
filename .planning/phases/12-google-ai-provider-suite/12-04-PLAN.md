---
phase: 12-google-ai-provider-suite
plan: 04
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - app/services/script_generator.py
  - app/services/trend_analyzer.py
  - requirements.txt
  - .env.example
autonomous: true

must_haves:
  truths:
    - "script_generator uses LLMProvider abstraction instead of direct Anthropic SDK"
    - "trend_analyzer uses LLMProvider abstraction instead of direct Anthropic SDK"
    - "Both services fall back to mock plan/analysis when USE_MOCK_DATA=true or no API key"
    - "google-generativeai package is in requirements.txt for Python 3.9 compatibility"
    - ".env.example documents GOOGLE_API_KEY and new provider type settings"
    - "Existing mock plan and mock analysis behavior is unchanged"
  artifacts:
    - path: "app/services/script_generator.py"
      provides: "LLM-agnostic script generator using LLMProvider"
      contains: "get_llm_provider"
    - path: "app/services/trend_analyzer.py"
      provides: "LLM-agnostic trend analyzer using LLMProvider"
      contains: "get_llm_provider"
    - path: "requirements.txt"
      provides: "google-generativeai dependency for Python 3.9"
      contains: "google-generativeai"
    - path: ".env.example"
      provides: "Google API configuration documentation"
      contains: "GOOGLE_API_KEY"
  key_links:
    - from: "app/services/script_generator.py"
      to: "app/services/llm_provider/__init__.py"
      via: "get_llm_provider() factory call"
      pattern: "get_llm_provider"
    - from: "app/services/trend_analyzer.py"
      to: "app/services/llm_provider/__init__.py"
      via: "get_llm_provider() factory call"
      pattern: "get_llm_provider"
---

<objective>
Refactor script_generator and trend_analyzer to use LLMProvider abstraction, and update requirements/env documentation.

Purpose: Replace direct Anthropic SDK usage in script_generator.py and trend_analyzer.py with the LLMProvider abstraction from Plan 01. This makes both services LLM-agnostic -- they work with Gemini, Claude, or mock based on config. Also add google-generativeai to requirements.txt and update .env.example with new settings.

Output: Refactored script_generator.py and trend_analyzer.py using LLMProvider, updated requirements.txt and .env.example.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-google-ai-provider-suite/12-RESEARCH.md
@.planning/phases/12-google-ai-provider-suite/12-01-SUMMARY.md
@app/services/script_generator.py
@app/services/trend_analyzer.py
@app/services/llm_provider/base.py
@app/services/llm_provider/__init__.py
@requirements.txt
@.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: Refactor script_generator.py to use LLMProvider</name>
  <files>
    app/services/script_generator.py
  </files>
  <action>
Refactor `generate_production_plan()` and its private helpers to use LLMProvider instead of direct Anthropic SDK. Key changes:

1. **Replace the mock/real decision logic:**
   - OLD: `if settings.use_mock_data or not settings.anthropic_api_key:` then mock, else `_generate_claude_plan()`
   - NEW: `if settings.use_mock_data:` then mock, else use LLMProvider (which handles its own mock fallback)
   - The LLMProvider factory already handles missing API keys by returning MockLLMProvider

2. **Replace `_generate_claude_plan()` with `_generate_llm_plan()`:**
   - Remove `from anthropic import Anthropic` import
   - Remove `from app.services.trend_analyzer import _add_additional_properties_false` import
   - Add `from app.services.llm_provider import get_llm_provider`
   - Get provider: `llm = get_llm_provider()`

   **Call 1 (analysis):** Use `generate_text()`:
   ```python
   analysis_text = llm.generate_text(
       prompt=analysis_prompt,
       temperature=0.8,
       max_tokens=4096
   )
   ```

   **Call 2 (structured output):** Use `generate_structured()`:
   ```python
   result = llm.generate_structured(
       prompt=structured_prompt,
       schema=VideoProductionPlanCreate,
       temperature=0.7
   )
   plan_data = result.model_dump()
   ```

   This replaces the Claude-specific tool-use pattern with LLMProvider's schema-based structured output (which works for both Gemini native JSON mode and Claude tool-use, depending on which provider is active).

3. **Keep all existing code untouched:**
   - ANALYSIS_PROMPT and STRUCTURED_OUTPUT_PROMPT templates stay the same
   - `_format_theme_info()`, `_format_content_refs()`, `_format_trend_section()` stay the same
   - `_generate_mock_plan()` stays the same
   - `save_production_plan()` stays the same
   - Error handling: wrap `_generate_llm_plan()` in try/except, fall back to mock on failure (same as current pattern)

4. **Update module docstring:** Change "Claude-powered script generator" to "LLM-powered script generator with provider abstraction"

5. **Simplify structured output prompt:** Remove the "use the create_production_plan tool" instruction from STRUCTURED_OUTPUT_PROMPT since the LLMProvider handles structured output natively. Change it to: "Create the final Video Production Plan with these exact fields:" (the schema enforcement is handled by the provider, not by prompt engineering).
  </action>
  <verify>
Run:
```python
python -c "
from app.services.script_generator import generate_production_plan, _generate_mock_plan

# Test mock plan still works
theme = {'theme': 'test', 'product_name': 'TestProduct', 'tagline': 'Best ever', 'video_duration_seconds': 20}
refs = [{'title': 'Ref1', 'description': 'Test ref', 'talking_points': ['Point 1', 'Point 2']}]
plan = generate_production_plan(theme, refs)
print(f'Plan title: {plan[\"title\"]}')
print(f'Scenes: {len(plan[\"scenes\"])}')
print(f'Has voiceover: {bool(plan.get(\"voiceover_script\"))}')
print('Script generator refactored successfully')
"
```
  </verify>
  <done>script_generator.py uses LLMProvider abstraction. No direct Anthropic SDK imports. Mock plan generation unchanged. LLM plan generation uses generate_text() for analysis and generate_structured() for structured output.</done>
</task>

<task type="auto">
  <name>Task 2: Refactor trend_analyzer.py and update requirements/env</name>
  <files>
    app/services/trend_analyzer.py
    requirements.txt
    .env.example
  </files>
  <action>
**trend_analyzer.py** - Refactor to use LLMProvider:

1. **Replace the real mode section:**
   - Remove `from anthropic import Anthropic` import (currently inside try block)
   - Remove `from tenacity import retry, stop_after_attempt, wait_exponential` import (LLMProvider handles retries internally)
   - Add `from app.services.llm_provider import get_llm_provider` at top of file
   - Keep `_extract_top_hashtags()` and `_add_additional_properties_false()` functions unchanged (the latter may still be useful for other consumers)

2. **In `analyze_trends()` real mode section:**
   - Replace the `try` block's Claude-specific code with:
   ```python
   llm = get_llm_provider()
   # Build prompt (same prompt text as existing)
   # ... existing prompt construction stays ...
   result = llm.generate_structured(
       prompt=prompt,
       schema=TrendReportCreate,
       temperature=0.7
   )
   return result.model_dump()
   ```
   - Remove the `call_claude()` inner function and tool-use extraction logic
   - Keep the error handling: `except Exception as exc:` with fallback to mock analysis

3. **Keep all existing code untouched:**
   - Mock analysis block stays the same
   - `_extract_top_hashtags()` stays the same
   - `_add_additional_properties_false()` stays the same (other modules may import it)
   - The prompt text stays the same (remove "Use the generate_trend_report tool" instruction, replace with "Provide your analysis with these exact fields:")

4. **Update module docstring:** Change "Trend analysis using Claude API" to "Trend analysis using LLM provider abstraction"

**requirements.txt** - Add Google AI SDK:
- Add line after `anthropic`: `google-generativeai>=0.8.0,<1.0  # Python 3.9 compatible (deprecated, use google-genai for Python 3.10+)`
- Keep all existing dependencies unchanged

**.env.example** - Add Google AI settings:
- Add after the `HEYGEN_API_KEY=` line:
  ```
  GOOGLE_API_KEY=                # Google AI Studio key (Gemini + Imagen + Veo)
  ```
- Add to the "Provider Selection" section:
  ```
  LLM_PROVIDER_TYPE=mock         # mock/gemini
  IMAGE_PROVIDER_TYPE=mock       # mock/imagen
  ```
- Update `VIDEO_PROVIDER_TYPE` comment to include veo:
  ```
  VIDEO_PROVIDER_TYPE=mock       # mock/svd/kling/minimax/veo
  ```
  </action>
  <verify>
Run:
```python
python -c "
from app.services.trend_analyzer import analyze_trends, _extract_top_hashtags

# Test mock analysis
trends = [
    {'title': 'Test Video 1', 'platform': 'tiktok', 'likes': 1000, 'comments': 50, 'shares': 20, 'engagement_velocity': 500.0, 'hashtags': ['#test', '#viral']},
    {'title': 'Test Video 2', 'platform': 'youtube', 'likes': 2000, 'comments': 100, 'shares': 40, 'engagement_velocity': 800.0, 'hashtags': ['#test', '#trending']},
]
result = analyze_trends(trends)
print(f'Analyzed count: {result[\"analyzed_count\"]}')
print(f'Video styles: {len(result[\"video_styles\"])}')
print(f'Patterns: {len(result[\"common_patterns\"])}')
print(f'Recommendations: {len(result[\"recommendations\"])}')
print('Trend analyzer refactored successfully')
"
```

Also verify requirements.txt:
```bash
grep -c "google-generativeai" requirements.txt
```
Should return 1.

Also verify .env.example:
```bash
grep -c "GOOGLE_API_KEY" .env.example
```
Should return 1.
  </verify>
  <done>trend_analyzer.py uses LLMProvider abstraction. requirements.txt includes google-generativeai for Python 3.9. .env.example documents GOOGLE_API_KEY, LLM_PROVIDER_TYPE, IMAGE_PROVIDER_TYPE, and updated VIDEO_PROVIDER_TYPE with veo option.</done>
</task>

</tasks>

<verification>
1. `python -c "from app.services.script_generator import generate_production_plan; print('OK')"` succeeds
2. `python -c "from app.services.trend_analyzer import analyze_trends; print('OK')"` succeeds
3. Neither file imports `anthropic` directly (LLMProvider handles that)
4. Mock mode produces identical output to before refactoring
5. `grep "google-generativeai" requirements.txt` finds the dependency
6. `grep "GOOGLE_API_KEY" .env.example` finds the config documentation
7. `grep "LLM_PROVIDER_TYPE" .env.example` finds the provider type setting
</verification>

<success_criteria>
- script_generator.py uses get_llm_provider() instead of Anthropic SDK directly
- trend_analyzer.py uses get_llm_provider() instead of Anthropic SDK directly
- Mock plan/analysis generation is unchanged (backward compatible)
- google-generativeai added to requirements.txt with Python 3.9 version pin
- .env.example documents all new Google AI settings
- No breaking changes to existing provider configurations
</success_criteria>

<output>
After completion, create `.planning/phases/12-google-ai-provider-suite/12-04-SUMMARY.md`
</output>
