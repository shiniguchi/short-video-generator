---
phase: 01-foundation-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - Dockerfile
  - .env.example
  - requirements.txt
  - app/config.py
  - config/sample-data.yml
autonomous: true

must_haves:
  truths:
    - "All services start successfully via `docker-compose up`"
    - "Environment variables load from .env file"
    - "Local config file provides sample data fallback"
  artifacts:
    - path: "docker-compose.yml"
      provides: "Multi-service orchestration with PostgreSQL, Redis, web, worker"
      min_lines: 60
      contains: "depends_on.*service_healthy"
    - path: "Dockerfile"
      provides: "Shared Python image for web and worker services"
      min_lines: 15
      contains: "FROM python:3.11-slim"
    - path: "app/config.py"
      provides: "Pydantic settings with .env loading"
      min_lines: 20
      exports: ["Settings", "get_settings"]
    - path: "config/sample-data.yml"
      provides: "Local fallback data when Google Sheets unavailable"
      contains: "theme:"
  key_links:
    - from: "app/config.py"
      to: ".env"
      via: "pydantic-settings auto-loading"
      pattern: "model_config.*env_file"
    - from: "docker-compose.yml"
      to: "Dockerfile"
      via: "build context"
      pattern: "build:.*context"
    - from: "docker-compose.yml"
      to: "PostgreSQL healthcheck"
      via: "depends_on service_healthy"
      pattern: "postgres:.*condition: service_healthy"
---

<objective>
Establish Docker Compose infrastructure with all core services, environment-based configuration, and local data fallback.

Purpose: Create the foundation for all subsequent services to run in isolated containers with health-checked startup ordering.
Output: Fully orchestrated multi-service Docker environment with config management and sample data.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-infrastructure/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Create Docker Compose multi-service orchestration</name>
  <files>
    docker-compose.yml
    Dockerfile
    .env.example
    requirements.txt
    .dockerignore
  </files>
  <action>
Create docker-compose.yml with 4 services:
- **postgres**: PostgreSQL 16-alpine with named volume `postgres_data`, healthcheck using `pg_isready`, environment variables from .env (POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB), port 5432 exposed
- **redis**: Redis 7-alpine with healthcheck using `redis-cli ping`, port 6379 exposed
- **web**: FastAPI service, build from Dockerfile, depends_on postgres and redis with `condition: service_healthy`, command `uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload`, port 8000:8000, volume mount for live code reload (./app:/app/app)
- **celery-worker**: Celery worker service, same build as web, depends_on postgres and redis with `condition: service_healthy`, command `celery -A app.worker.celery_app worker --loglevel=info`

Create Dockerfile (shared by web and celery-worker):
- FROM python:3.11-slim
- Install system dependencies: postgresql-client (for pg_isready if needed), build-essential
- WORKDIR /app
- COPY requirements.txt and RUN pip install
- COPY ./app /app/app (app code)
- No CMD (specified in docker-compose.yml)

Create requirements.txt with exact versions from research:
- fastapi[standard]>=0.129.0
- celery[redis]>=5.6.2
- sqlalchemy[asyncio]>=2.0
- alembic>=1.18
- asyncpg
- pydantic-settings
- python-dotenv
- uvicorn[standard]

Create .env.example template (user copies to .env):
```
POSTGRES_USER=viralforge
POSTGRES_PASSWORD=dev_password_change_in_production
POSTGRES_DB=viralforge
DATABASE_URL=postgresql+asyncpg://viralforge:dev_password_change_in_production@postgres:5432/viralforge

REDIS_URL=redis://redis:6379/0
CELERY_BROKER_URL=redis://redis:6379/0
CELERY_RESULT_BACKEND=redis://redis:6379/0

API_SECRET_KEY=dev_secret_key_change_in_production
```

Create .dockerignore:
```
.git
.env
__pycache__
*.pyc
.planning
.vscode
```

Use named volumes (not bind mounts) for PostgreSQL data to avoid permission issues. Research warns about UID/GID conflicts with bind mounts.

Health checks prevent race conditions during startup per research pattern 4.
  </action>
  <verify>
Run `docker compose up -d` and verify all services reach healthy status: `docker compose ps` shows all services with STATE "running" or "Up" and STATUS includes "(healthy)".
  </verify>
  <done>
All 4 services start successfully, health checks pass, logs visible via `docker compose logs`, no connection errors.
  </done>
</task>

<task type="auto">
  <name>Create pydantic-settings configuration system</name>
  <files>
    app/__init__.py
    app/config.py
  </files>
  <action>
Create app/__init__.py (empty file to make app a package).

Create app/config.py using pydantic-settings pattern from research:

```python
from pydantic_settings import BaseSettings, SettingsConfigDict
from functools import lru_cache

class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False
    )

    # Database
    database_url: str

    # Redis
    redis_url: str = "redis://redis:6379/0"

    # Celery
    celery_broker_url: str = "redis://redis:6379/0"
    celery_result_backend: str = "redis://redis:6379/0"

    # API
    api_secret_key: str

    # Local config fallback
    local_config_path: str = "config/sample-data.yml"

@lru_cache()
def get_settings():
    return Settings()
```

Use exact pattern from research (Pattern 1) including lru_cache for singleton behavior. Do NOT manually call load_dotenv() — pydantic-settings handles this automatically.
  </action>
  <verify>
Create test script: `python -c "from app.config import get_settings; s = get_settings(); print(s.database_url)"` — should print DATABASE_URL from .env without errors.
  </verify>
  <done>
Settings class loads all environment variables, get_settings() returns cached singleton, no import errors.
  </done>
</task>

<task type="auto">
  <name>Create local config file fallback</name>
  <files>
    config/sample-data.yml
  </files>
  <action>
Create config/sample-data.yml with sample data structure matching Google Sheets schema (future implementation):

```yaml
# Sample data for local development (used when Google Sheets unavailable)

# Config sheet equivalent
config:
  theme: "Product Demo"
  target_platform: "tiktok"
  video_duration_seconds: 20
  style: "cinematic"
  auto_post: false
  trend_scrape_schedule: "0 */6 * * *"  # Every 6 hours

# Product/Content References sheet equivalent
content_references:
  - ref_id: "demo-01"
    type: "product"
    title: "Smart Water Bottle"
    description: "Self-cleaning UV water bottle with temperature display"
    media_url: "https://example.com/product-image.jpg"
    talking_points:
      - "UV-C LED self-cleaning every 2 hours"
      - "Temperature display on cap"
      - "Keeps drinks cold for 24 hours"
```

This file is READ by the system when Google Sheets credentials are not configured. It provides enough structure for testing the pipeline without external dependencies.
  </action>
  <verify>
Verify file exists and is valid YAML: `python -c "import yaml; yaml.safe_load(open('config/sample-data.yml'))"`
  </verify>
  <done>
config/sample-data.yml exists, contains valid YAML, includes all required fields for pipeline testing.
  </done>
</task>

</tasks>

<verification>
1. Run `docker compose up` and verify all services start without errors
2. Check `docker compose ps` shows all services healthy
3. Verify environment variables load: `docker compose exec web python -c "from app.config import get_settings; print(get_settings().database_url)"`
4. Verify local config file is valid YAML and readable
5. Check logs show no connection errors: `docker compose logs --tail=50`
</verification>

<success_criteria>
- `docker compose up` starts all 4 services successfully
- PostgreSQL and Redis health checks pass within 30 seconds
- Web and worker services start and show "ready" or "connected" in logs
- app/config.py loads all environment variables without errors
- config/sample-data.yml contains valid YAML structure
- No "connection refused" or "permission denied" errors in logs
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-infrastructure/01-01-SUMMARY.md`
</output>
