---
phase: 01-foundation-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - app/models.py
  - app/database.py
  - alembic.ini
  - alembic/env.py
  - alembic/versions/
autonomous: true

must_haves:
  truths:
    - "PostgreSQL schema is created via migrations"
    - "Database tables exist for jobs, trends, scripts, video metadata"
    - "Async sessions connect to database without errors"
  artifacts:
    - path: "app/models.py"
      provides: "SQLAlchemy async models for all pipeline entities"
      min_lines: 80
      contains: "class Job.*Base"
    - path: "app/database.py"
      provides: "Async engine and session factory"
      min_lines: 25
      exports: ["engine", "async_session_factory", "get_session"]
    - path: "alembic/env.py"
      provides: "Async migration configuration"
      contains: "run_async_migrations"
    - path: "alembic/versions/"
      provides: "Initial migration file"
      contains: "*.py"
  key_links:
    - from: "app/database.py"
      to: "app/config.py"
      via: "settings.database_url"
      pattern: "get_settings\\(\\)\\.database_url"
    - from: "alembic/env.py"
      to: "app/models.py"
      via: "Base.metadata import"
      pattern: "from app.models import Base"
    - from: "app/models.py"
      to: "PostgreSQL"
      via: "SQLAlchemy async engine"
      pattern: "create_async_engine"
---

<objective>
Establish PostgreSQL database schema with async SQLAlchemy models and Alembic migrations for all pipeline entities.

Purpose: Create persistent storage for jobs, trends, scripts, and video metadata with async-first database operations.
Output: Fully migrated database schema with async session management ready for API endpoints.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation-infrastructure/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Create SQLAlchemy async models</name>
  <files>
    app/models.py
    app/database.py
  </files>
  <action>
Create app/models.py with async SQLAlchemy 2.0 models for pipeline entities:

```python
from sqlalchemy import Column, Integer, String, Text, DateTime, Float, Boolean, JSON, ForeignKey
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.sql import func
from datetime import datetime

Base = declarative_base()

class Job(Base):
    """Pipeline execution job tracking"""
    __tablename__ = "jobs"

    id = Column(Integer, primary_key=True)
    status = Column(String(50), nullable=False, default="pending")  # pending, running, completed, failed
    stage = Column(String(50))  # current pipeline stage
    theme = Column(String(255))
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    error_message = Column(Text)
    metadata = Column(JSON)  # Flexible storage for stage-specific data

class Trend(Base):
    """Collected trending videos from TikTok/YouTube"""
    __tablename__ = "trends"

    id = Column(Integer, primary_key=True)
    platform = Column(String(50), nullable=False)  # tiktok, youtube
    external_id = Column(String(255), nullable=False, unique=True)  # Platform's video ID
    title = Column(String(500))
    creator = Column(String(255))
    hashtags = Column(JSON)  # Array of hashtag strings
    views = Column(Integer)
    likes = Column(Integer)
    comments = Column(Integer)
    shares = Column(Integer)
    video_url = Column(String(1000))
    thumbnail_url = Column(String(1000))
    collected_at = Column(DateTime(timezone=True), server_default=func.now())
    metadata = Column(JSON)

class Script(Base):
    """Generated video production plans (scripts)"""
    __tablename__ = "scripts"

    id = Column(Integer, primary_key=True)
    job_id = Column(Integer, ForeignKey("jobs.id"))
    video_prompt = Column(Text, nullable=False)
    scenes = Column(JSON, nullable=False)  # Array of scene objects
    text_overlays = Column(JSON)  # Array of text overlay objects
    voiceover_script = Column(Text)
    title = Column(String(500))
    description = Column(Text)
    hashtags = Column(JSON)
    created_at = Column(DateTime(timezone=True), server_default=func.now())

class Video(Base):
    """Generated video metadata and file paths"""
    __tablename__ = "videos"

    id = Column(Integer, primary_key=True)
    job_id = Column(Integer, ForeignKey("jobs.id"))
    script_id = Column(Integer, ForeignKey("scripts.id"))
    status = Column(String(50), default="generated")  # generated, approved, rejected, published
    file_path = Column(String(1000))
    thumbnail_path = Column(String(1000))
    duration_seconds = Column(Float)
    cost_usd = Column(Float)  # Total generation cost
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    approved_at = Column(DateTime(timezone=True))
    published_at = Column(DateTime(timezone=True))
    published_url = Column(String(1000))
    metadata = Column(JSON)
```

Create app/database.py with async session management per research Pattern 2:

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from typing import AsyncGenerator
from app.config import get_settings

settings = get_settings()

engine = create_async_engine(
    settings.database_url,
    echo=True,
    future=True,
    pool_pre_ping=True,
    pool_size=5
)

async_session_factory = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False  # CRITICAL: Prevents implicit queries after commit
)

async def get_session() -> AsyncGenerator[AsyncSession, None]:
    async with async_session_factory() as session:
        yield session
```

Use expire_on_commit=False per research critical pitfall — without this, async sessions fail with runtime errors.
  </action>
  <verify>
Test imports: `docker compose exec web python -c "from app.models import Base, Job, Trend, Script, Video; from app.database import engine; print('Models loaded successfully')"`
  </verify>
  <done>
All models import without errors, Base.metadata contains 4 tables, async engine connects to database.
  </done>
</task>

<task type="auto">
  <name>Initialize Alembic with async template</name>
  <files>
    alembic.ini
    alembic/env.py
    alembic/script.py.mako
    alembic/README
  </files>
  <action>
Initialize Alembic with async template inside web container:

```bash
docker compose exec web alembic init -t async alembic
```

Edit alembic/env.py to:
1. Import app models and set target_metadata
2. Use database URL from settings

Replace the target_metadata line with:
```python
from app.models import Base
from app.config import get_settings

settings = get_settings()
config.set_main_option("sqlalchemy.url", settings.database_url)

target_metadata = Base.metadata
```

This ensures Alembic autogenerate detects all model changes. Research Pitfall 3 warns that missing imports cause empty migrations.

Keep the async template's run_async_migrations() function unchanged — it handles async engine properly.
  </action>
  <verify>
Run `docker compose exec web alembic current` — should show "No revision" (database empty but Alembic configured).
  </verify>
  <done>
Alembic initialized with async template, env.py imports models, configuration loads database URL from settings.
  </done>
</task>

<task type="auto">
  <name>Create and run initial migration</name>
  <files>
    alembic/versions/001_initial_schema.py
  </files>
  <action>
Generate initial migration inside web container:

```bash
docker compose exec web alembic revision --autogenerate -m "initial schema"
```

Verify the generated migration file in alembic/versions/ contains create_table statements for jobs, trends, scripts, videos.

Run migration:

```bash
docker compose exec web alembic upgrade head
```

Verify tables created:

```bash
docker compose exec postgres psql -U viralforge -d viralforge -c "\dt"
```

Should list: jobs, trends, scripts, videos, alembic_version tables.

Research warns (Pitfall 3): Always run `alembic upgrade head` before autogenerate to ensure database is current. For initial migration this doesn't apply, but for future migrations this is critical.
  </action>
  <verify>
Run `docker compose exec postgres psql -U viralforge -d viralforge -c "SELECT table_name FROM information_schema.tables WHERE table_schema='public';"` — should list all 5 tables (4 models + alembic_version).
  </verify>
  <done>
Migration file exists, alembic upgrade head completes without errors, all tables created in PostgreSQL.
  </done>
</task>

</tasks>

<verification>
1. Verify models import: `docker compose exec web python -c "from app.models import Job, Trend, Script, Video; print('OK')"`
2. Verify async session: `docker compose exec web python -c "from app.database import get_session; print('OK')"`
3. Verify tables exist: `docker compose exec postgres psql -U viralforge -d viralforge -c "\dt"`
4. Verify alembic current: `docker compose exec web alembic current` shows migration ID
5. Test insert: `docker compose exec postgres psql -U viralforge -d viralforge -c "INSERT INTO jobs (status, theme) VALUES ('pending', 'test'); SELECT * FROM jobs;"`
</verification>

<success_criteria>
- All SQLAlchemy models import without errors
- Async engine connects to PostgreSQL
- Alembic generates migration with all 4 tables
- Migration runs successfully via `alembic upgrade head`
- PostgreSQL contains jobs, trends, scripts, videos, alembic_version tables
- Test insert into jobs table succeeds
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-infrastructure/01-02-SUMMARY.md`
</output>
