---
phase: 02-trend-intelligence
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - app/services/trend_analyzer.py
  - app/services/trend_reporter.py
  - app/tasks.py
  - app/api/routes.py
  - app/worker.py
autonomous: true

must_haves:
  truths:
    - "Claude API analyzes collected trends and produces structured TrendReport JSON"
    - "Analysis includes video style classifications (cinematic, talking-head, montage, text-heavy)"
    - "Analysis includes engagement velocity scores"
    - "Analysis includes pattern extraction (format, duration, hook type, text overlay, audio type)"
    - "TrendReport stored in database with all structured fields"
    - "Mock analysis mode works without Claude API key"
    - "Celery Beat schedule configured for periodic collection and analysis"
  artifacts:
    - path: "app/services/trend_analyzer.py"
      provides: "Claude API analysis with structured outputs and mock mode"
      contains: "def analyze_trends"
    - path: "app/services/trend_reporter.py"
      provides: "TrendReport DB storage and retrieval"
      contains: "async def save_report"
    - path: "app/tasks.py"
      provides: "Analysis Celery task + Beat schedule"
      contains: "def analyze_trends_task"
    - path: "app/api/routes.py"
      provides: "Trend report API endpoints"
      contains: "trend-reports"
  key_links:
    - from: "app/services/trend_analyzer.py"
      to: "app/schemas.py"
      via: "uses TrendReportCreate schema for Claude output validation"
      pattern: "TrendReportCreate"
    - from: "app/services/trend_reporter.py"
      to: "app/models.py"
      via: "stores TrendReport in database"
      pattern: "from app\\.models import TrendReport"
    - from: "app/tasks.py"
      to: "app/services/trend_analyzer.py"
      via: "Celery task calls analyzer"
      pattern: "analyze_trends"
    - from: "app/worker.py"
      to: "app/tasks.py"
      via: "Celery Beat schedule references task names"
      pattern: "beat_schedule"
---

<objective>
Build the trend analysis pipeline: Claude API integration for pattern analysis with structured outputs, TrendReport database storage, analysis Celery task, Celery Beat scheduling, and report API endpoints. Includes mock analysis mode for local testing without API keys.

Purpose: This is the "analyze" half of trend intelligence -- turning raw collected data into actionable pattern insights that Phase 3 (Content Generation) will consume to guide script creation.
Output: Working analysis pipeline, scheduled collection+analysis, report storage and retrieval.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-trend-intelligence/02-RESEARCH.md
@.planning/phases/02-trend-intelligence/02-01-SUMMARY.md
@.planning/phases/02-trend-intelligence/02-02-SUMMARY.md

@app/models.py
@app/schemas.py
@app/config.py
@app/database.py
@app/worker.py
@app/tasks.py
@app/services/trend_collector.py
@app/services/engagement.py
@app/api/routes.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Claude trend analyzer with mock mode and report storage</name>
  <files>
    app/services/trend_analyzer.py
    app/services/trend_reporter.py
  </files>
  <action>
**1. Create app/services/trend_analyzer.py:**

Implement `analyze_trends(trends: List[Dict]) -> Dict`:

This function takes a list of trend dicts (from DB), sends them to Claude for analysis, and returns a validated TrendReportCreate dict.

**Mock mode** (when `settings.use_mock_data` is True OR `settings.anthropic_api_key` is empty):
- Return a realistic hardcoded mock report:
```python
mock_report = {
    "analyzed_count": len(trends),
    "video_styles": [
        {"category": "talking-head", "confidence": 0.85, "count": max(1, len(trends) // 3)},
        {"category": "montage", "confidence": 0.78, "count": max(1, len(trends) // 4)},
        {"category": "text-heavy", "confidence": 0.72, "count": max(1, len(trends) // 5)},
        {"category": "cinematic", "confidence": 0.65, "count": max(1, len(trends) // 6)},
    ],
    "common_patterns": [
        {
            "format_description": "Hook question in first 2 seconds followed by rapid montage",
            "avg_duration_seconds": 28.5,
            "hook_type": "question",
            "uses_text_overlay": True,
            "audio_type": "trending-sound"
        },
        {
            "format_description": "Step-by-step tutorial with text overlays",
            "avg_duration_seconds": 45.0,
            "hook_type": "tutorial",
            "uses_text_overlay": True,
            "audio_type": "voiceover"
        },
        {
            "format_description": "Reaction or commentary on trending topic",
            "avg_duration_seconds": 35.0,
            "hook_type": "shock",
            "uses_text_overlay": False,
            "audio_type": "original"
        },
    ],
    "avg_engagement_velocity": sum(t.get('engagement_velocity', 0) for t in trends) / max(len(trends), 1),
    "top_hashtags": _extract_top_hashtags(trends, limit=10),
    "recommendations": [
        "Use hook questions in first 2 seconds to capture attention",
        "Keep videos between 25-45 seconds for optimal engagement",
        "Add text overlays for accessibility and silent viewing",
        "Use trending sounds when available for algorithm boost",
    ]
}
```
- Log: "Using mock analysis (no API key configured)"

**Real mode** (when `settings.use_mock_data` is False AND `settings.anthropic_api_key` is set):
- Import `anthropic` and create client: `Anthropic(api_key=settings.anthropic_api_key)`
- Prepare trend summary string from the input trends (limit to top 50 by engagement velocity):
  - For each trend: "Title: {title} | Platform: {platform} | Duration: {duration}s | Likes: {likes}, Comments: {comments}, Shares: {shares} | Velocity: {engagement_velocity:.1f}/hr | Hashtags: {hashtags joined} | Creator: {creator}"
- Build the Claude structured output schema from TrendReportCreate.model_json_schema()
  - IMPORTANT: Must set `additionalProperties: false` on all object types in the schema (Claude requirement)
  - Use a helper function to recursively add `additionalProperties: false` to all object definitions
- Call Claude API:
  ```python
  response = client.messages.create(
      model="claude-sonnet-4-20250514",  # Use Sonnet for cost efficiency on analysis
      max_tokens=4096,
      messages=[{"role": "user", "content": prompt}],
      # Use tool_use pattern for structured output (more reliable than output_config for complex schemas)
  )
  ```
  - Actually, for reliability, use the **tool_use pattern** instead of output_config: define a tool called "generate_trend_report" with the TrendReportCreate schema as input_schema, then extract `tool_use` block from response. This is more widely supported and avoids schema complexity issues.
  - Wrap with tenacity retry: `stop_after_attempt(3)`, `wait_exponential(multiplier=2, min=4, max=30)` for API transient errors
- Parse and validate response with TrendReportCreate Pydantic model
- Return validated dict

**Helper function `_extract_top_hashtags(trends, limit=10) -> List[str]`:**
- Flatten all hashtags from all trends
- Count occurrences using `collections.Counter`
- Return top N most common

**Helper function `_add_additional_properties_false(schema: dict) -> dict`:**
- Recursively walk the JSON schema
- For every object with `type: "object"` and `properties`, add `additionalProperties: false`
- Handle `$defs` / `definitions` references
- Return modified schema

All imports must be Python 3.9 compatible. Use `from typing import List, Dict, Any, Optional`.

**2. Create app/services/trend_reporter.py:**

Implement async functions for TrendReport DB operations:

`async def save_report(report_data: Dict, date_range_start: datetime, date_range_end: datetime) -> int`:
- Create TrendReport model instance from report_data dict
- Set date_range_start, date_range_end, and raw_report (full report dict for debugging)
- Use async session from `app.database.async_session_factory`
- Add and commit
- Return the report ID

`async def get_latest_report() -> Optional[Dict]`:
- Query TrendReport ordered by created_at DESC, limit 1
- Return as dict or None

`async def get_reports(limit: int = 10) -> List[Dict]`:
- Query TrendReport ordered by created_at DESC, limit N
- Return as list of dicts

`async def get_trends_for_analysis(hours: int = 24) -> List[Dict]`:
- Query Trend table for trends collected in last N hours
- Order by engagement_velocity DESC
- Limit to 100 (to keep Claude context manageable)
- Return as list of dicts with all fields
- Convert SQLAlchemy model instances to dicts (use `{c.name: getattr(trend, c.name) for c in Trend.__table__.columns}`)

All functions use `async with async_session_factory() as session:` pattern.
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
import asyncio
from app.services.trend_analyzer import analyze_trends

# Test mock analysis
mock_trends = [
    {'title': 'Test Video', 'platform': 'tiktok', 'likes': 1000, 'comments': 200, 'shares': 50, 'engagement_velocity': 250.0, 'hashtags': ['test', 'viral'], 'duration': 30, 'creator': 'test_user'},
    {'title': 'Another Video', 'platform': 'youtube', 'likes': 5000, 'comments': 800, 'shares': 0, 'engagement_velocity': 580.0, 'hashtags': ['cooking', 'recipe'], 'duration': 45, 'creator': 'chef_user'},
]

report = analyze_trends(mock_trends)
print(f'Report keys: {list(report.keys())}')
assert 'video_styles' in report
assert 'common_patterns' in report
assert 'avg_engagement_velocity' in report
assert 'top_hashtags' in report
assert len(report['video_styles']) > 0
assert report['video_styles'][0]['category'] in ('talking-head', 'montage', 'text-heavy', 'cinematic', 'animation')
print(f'Styles: {[s[\"category\"] for s in report[\"video_styles\"]]}')
print(f'Patterns: {len(report[\"common_patterns\"])}')
print('Mock analysis OK')
"` -- mock analysis returns valid report.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
import asyncio
from datetime import datetime, timezone, timedelta
from app.services.trend_reporter import save_report, get_latest_report

# First collect some trends
from app.services.trend_collector import collect_all_trends
asyncio.run(collect_all_trends())

# Test report storage
report_data = {
    'analyzed_count': 20,
    'video_styles': [{'category': 'talking-head', 'confidence': 0.85, 'count': 8}],
    'common_patterns': [{'format_description': 'Hook question', 'avg_duration_seconds': 30.0, 'hook_type': 'question', 'uses_text_overlay': True, 'audio_type': 'voiceover'}],
    'avg_engagement_velocity': 350.5,
    'top_hashtags': ['viral', 'trending'],
    'recommendations': ['Use hooks']
}
now = datetime.now(timezone.utc)
report_id = asyncio.run(save_report(report_data, now - timedelta(hours=24), now))
print(f'Saved report ID: {report_id}')
assert report_id > 0

# Test retrieval
latest = asyncio.run(get_latest_report())
print(f'Latest report: {latest is not None}')
assert latest is not None
assert latest['analyzed_count'] == 20
print('Report storage and retrieval OK')
"` -- reports save and retrieve from DB.
  </verify>
  <done>
Claude analyzer produces structured TrendReport with style classifications, patterns, and recommendations. Mock mode returns realistic reports without API key. Reports stored in trend_reports table with full structured data. Retrieval functions work for latest and historical reports.
  </done>
</task>

<task type="auto">
  <name>Task 2: Analysis Celery task, Beat schedule, and report API endpoints</name>
  <files>
    app/tasks.py
    app/api/routes.py
    app/worker.py
  </files>
  <action>
**1. Update app/tasks.py -- add analyze_trends_task:**

```python
@celery_app.task(
    bind=True,
    name='app.tasks.analyze_trends_task',
    max_retries=3,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True,
)
def analyze_trends_task(self):
    """Analyze collected trends using Claude API."""
    logger.info(f"Starting trend analysis (attempt {self.request.retries + 1})")
    try:
        from app.services.trend_reporter import get_trends_for_analysis, save_report
        from app.services.trend_analyzer import analyze_trends
        from datetime import datetime, timezone, timedelta

        # Get trends from last 24 hours
        trends = asyncio.run(get_trends_for_analysis(hours=24))

        if not trends:
            logger.warning("No trends found for analysis")
            return {"status": "skipped", "reason": "No trends in last 24 hours"}

        # Analyze with Claude (or mock)
        report_data = analyze_trends(trends)

        # Save report to DB
        now = datetime.now(timezone.utc)
        report_id = asyncio.run(save_report(
            report_data=report_data,
            date_range_start=now - timedelta(hours=24),
            date_range_end=now
        ))

        logger.info(f"Trend analysis complete, report ID: {report_id}")
        return {
            "status": "success",
            "report_id": report_id,
            "analyzed_count": report_data.get("analyzed_count", 0)
        }
    except Exception as exc:
        logger.error(f"Trend analysis failed: {exc}")
        raise
```

**2. Update app/worker.py -- add Celery Beat schedule:**

After the `celery_app.conf.update(...)` block, add the beat schedule:

```python
from app.config import get_settings

settings = get_settings()

celery_app.conf.beat_schedule = {
    'collect-trends-periodic': {
        'task': 'app.tasks.collect_trends_task',
        'schedule': settings.trend_scrape_interval_hours * 3600,  # Convert hours to seconds
    },
    'analyze-trends-periodic': {
        'task': 'app.tasks.analyze_trends_task',
        'schedule': settings.trend_scrape_interval_hours * 3600,  # Same interval
        # Analysis runs at same interval but relies on collected data from last 24h
    },
}
```

NOTE: `settings` import is already at top of worker.py. Move the beat_schedule config to after the conf.update() call. The analysis task runs at the same interval as collection but independently queries the last 24h of data.

**3. Update app/api/routes.py -- add report endpoints:**

Add these endpoints (keep all existing ones):

```python
@router.post("/analyze-trends")
async def trigger_trend_analysis():
    """Trigger trend analysis with Claude API."""
    from app.tasks import analyze_trends_task
    task = analyze_trends_task.delay()
    return {"task_id": str(task.id), "status": "queued", "description": "Analyzing collected trends"}

@router.get("/trend-reports")
async def list_trend_reports(
    limit: int = 10,
    session: AsyncSession = Depends(get_session)
):
    """List trend analysis reports."""
    from sqlalchemy import select
    from app.models import TrendReport

    query = select(TrendReport).order_by(TrendReport.created_at.desc()).limit(limit)
    result = await session.execute(query)
    reports = result.scalars().all()

    return {
        "count": len(reports),
        "reports": [
            {
                "id": r.id,
                "analyzed_count": r.analyzed_count,
                "date_range_start": r.date_range_start.isoformat() if r.date_range_start else None,
                "date_range_end": r.date_range_end.isoformat() if r.date_range_end else None,
                "video_styles": r.video_styles,
                "common_patterns": r.common_patterns,
                "avg_engagement_velocity": r.avg_engagement_velocity,
                "top_hashtags": r.top_hashtags,
                "recommendations": r.recommendations,
                "created_at": r.created_at.isoformat() if r.created_at else None,
            }
            for r in reports
        ]
    }

@router.get("/trend-reports/latest")
async def get_latest_trend_report(session: AsyncSession = Depends(get_session)):
    """Get the most recent trend analysis report."""
    from sqlalchemy import select
    from app.models import TrendReport

    query = select(TrendReport).order_by(TrendReport.created_at.desc()).limit(1)
    result = await session.execute(query)
    report = result.scalars().first()

    if not report:
        from fastapi import HTTPException
        raise HTTPException(status_code=404, detail="No trend reports found")

    return {
        "id": report.id,
        "analyzed_count": report.analyzed_count,
        "date_range_start": report.date_range_start.isoformat() if report.date_range_start else None,
        "date_range_end": report.date_range_end.isoformat() if report.date_range_end else None,
        "video_styles": report.video_styles,
        "common_patterns": report.common_patterns,
        "avg_engagement_velocity": report.avg_engagement_velocity,
        "top_hashtags": report.top_hashtags,
        "recommendations": report.recommendations,
        "created_at": report.created_at.isoformat() if report.created_at else None,
    }
```

Ensure HTTPException is imported at the top of routes.py (it may already be -- check existing imports first).
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "from app.tasks import collect_trends_task, analyze_trends_task; print(f'Collection: {collect_trends_task.name}'); print(f'Analysis: {analyze_trends_task.name}'); print('Both tasks registered')"` -- both tasks import.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
from app.worker import celery_app
schedule = celery_app.conf.beat_schedule
print(f'Beat schedule: {list(schedule.keys())}')
assert 'collect-trends-periodic' in schedule
assert 'analyze-trends-periodic' in schedule
print(f'Collection interval: {schedule[\"collect-trends-periodic\"][\"schedule\"]}s')
print('Celery Beat schedule configured')
"` -- Beat schedule exists.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
import asyncio
from app.services.trend_collector import collect_all_trends
asyncio.run(collect_all_trends())

from fastapi.testclient import TestClient
from app.main import app
client = TestClient(app)

# Trigger analysis (runs synchronously in test client context)
# Instead, test the analysis flow directly
from app.services.trend_reporter import get_trends_for_analysis, save_report
from app.services.trend_analyzer import analyze_trends
from datetime import datetime, timezone, timedelta

trends = asyncio.run(get_trends_for_analysis(hours=24))
print(f'Trends for analysis: {len(trends)}')
assert len(trends) > 0

report_data = analyze_trends(trends)
now = datetime.now(timezone.utc)
report_id = asyncio.run(save_report(report_data, now - timedelta(hours=24), now))
print(f'Report saved: ID {report_id}')

# Test report endpoints
response = client.get('/trend-reports')
print(f'GET /trend-reports: {response.status_code}')
assert response.status_code == 200
data = response.json()
assert data['count'] > 0
print(f'Reports count: {data[\"count\"]}')

response = client.get('/trend-reports/latest')
print(f'GET /trend-reports/latest: {response.status_code}')
assert response.status_code == 200
data = response.json()
assert 'video_styles' in data
assert len(data['video_styles']) > 0
print(f'Latest report styles: {[s[\"category\"] for s in data[\"video_styles\"]]}')
print('Full analysis pipeline working')
"` -- end-to-end analysis flow works.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
from fastapi.testclient import TestClient
from app.main import app
client = TestClient(app)

# Verify all endpoints exist
for endpoint in ['/health', '/trends', '/trend-reports', '/trend-reports/latest']:
    r = client.get(endpoint)
    print(f'GET {endpoint}: {r.status_code}')

for endpoint in ['/test-task', '/collect-trends', '/analyze-trends']:
    r = client.post(endpoint)
    print(f'POST {endpoint}: {r.status_code}')

print('All endpoints accessible')
"` -- all endpoints respond.
  </verify>
  <done>
Analysis Celery task queries last 24h trends, runs Claude analysis (mock or real), and saves structured report. Celery Beat configured for periodic collection every 6 hours and analysis at same interval. API endpoints: POST /analyze-trends triggers analysis, GET /trend-reports lists reports, GET /trend-reports/latest returns most recent. Full pipeline works end-to-end with mock data.
  </done>
</task>

</tasks>

<verification>
1. Mock analysis returns valid TrendReport with style classifications and patterns
2. TrendReport saved to database with all structured fields
3. Analysis Celery task chains: query trends -> analyze -> save report
4. Celery Beat schedule has both collection and analysis tasks
5. API endpoints serve trend reports with full structured data
6. End-to-end: collect (mock) -> save -> analyze (mock) -> save report -> GET /trend-reports/latest returns complete report
7. All code Python 3.9 compatible
</verification>

<success_criteria>
- `analyze_trends(trends)` returns dict with video_styles, common_patterns, engagement velocity, top_hashtags, recommendations
- Style categories include at least: talking-head, montage, text-heavy, cinematic
- Report stored in trend_reports table and retrievable via API
- Celery Beat schedule includes both collect-trends-periodic and analyze-trends-periodic
- GET /trend-reports/latest returns structured report with all fields
- Mock mode works without any API keys configured
- Full end-to-end flow: collection -> analysis -> storage -> retrieval works with mock data
</success_criteria>

<output>
After completion, create `.planning/phases/02-trend-intelligence/02-03-SUMMARY.md`
</output>
