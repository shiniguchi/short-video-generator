---
phase: 02-trend-intelligence
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - app/models.py
  - app/schemas.py
  - app/config.py
  - alembic/versions/002_trend_intelligence_schema.py
  - app/scrapers/__init__.py
  - app/scrapers/mock_data/tiktok_trending.json
  - app/scrapers/mock_data/youtube_shorts.json
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "Trend model has all required metadata fields (title, description, hashtags, engagement counts, duration, creator info, sound name, thumbnail URL, posted_at)"
    - "TrendReport model exists for storing structured analysis results"
    - "Composite unique constraint on (platform, external_id) prevents duplicate trends"
    - "Mock data fixtures exist with realistic TikTok and YouTube Shorts data"
    - "USE_MOCK_DATA config flag defaults to true for local development"
  artifacts:
    - path: "app/models.py"
      provides: "Updated Trend model + new TrendReport model"
      contains: "class TrendReport"
    - path: "app/schemas.py"
      provides: "Pydantic schemas for scrapers and analysis"
      contains: "class TrendCreate"
    - path: "app/config.py"
      provides: "Settings with USE_MOCK_DATA, API keys"
      contains: "use_mock_data"
    - path: "alembic/versions/002_trend_intelligence_schema.py"
      provides: "Migration adding new columns and TrendReport table"
      contains: "def upgrade"
    - path: "app/scrapers/mock_data/tiktok_trending.json"
      provides: "10 realistic mock TikTok trends"
      min_lines: 50
    - path: "app/scrapers/mock_data/youtube_shorts.json"
      provides: "10 realistic mock YouTube Shorts"
      min_lines: 50
  key_links:
    - from: "alembic/versions/002_trend_intelligence_schema.py"
      to: "app/models.py"
      via: "migration matches model columns"
      pattern: "op\\.add_column|op\\.create_table"
    - from: "app/schemas.py"
      to: "app/models.py"
      via: "schema fields match model columns"
      pattern: "class Trend"
---

<objective>
Establish the data foundation for trend intelligence: evolve the database schema to support full trend metadata and analysis reports, create Pydantic validation schemas, add configuration for mock/real API switching, and provide realistic mock data fixtures.

Purpose: Every subsequent plan (scrapers, analysis, scheduling) depends on having the correct schema, validation models, config flags, and test data in place.
Output: Updated models, migration, schemas, config, mock data fixtures, updated requirements.txt
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-trend-intelligence/02-RESEARCH.md

@app/models.py
@app/config.py
@app/database.py
@alembic/versions/001_initial_schema.py
@requirements.txt
@.env
</context>

<tasks>

<task type="auto">
  <name>Task 1: Evolve Trend model, add TrendReport model, create migration, update config</name>
  <files>
    app/models.py
    app/config.py
    alembic/versions/002_trend_intelligence_schema.py
    requirements.txt
    .env
  </files>
  <action>
**1. Update app/models.py -- Trend model additions:**

Add these columns to the existing Trend class (keeping all existing columns):
- `description = Column(Text)` -- video description text
- `duration = Column(Integer)` -- video duration in seconds
- `creator_id = Column(String(255))` -- platform-specific creator ID
- `sound_name = Column(String(500))` -- audio/sound used
- `posted_at = Column(DateTime(timezone=True))` -- when video was originally posted
- `engagement_velocity = Column(Float)` -- calculated (likes+comments+shares)/hours_since_posted

Change `external_id` unique constraint: Remove the simple `unique=True` on external_id column. Add a `__table_args__` with `UniqueConstraint('platform', 'external_id', name='uq_platform_external_id')` to the Trend class. This is the TREND-04 dedup requirement.

**Add TrendReport model (new class):**
```python
class TrendReport(Base):
    """AI-generated trend analysis reports"""
    __tablename__ = "trend_reports"

    id = Column(Integer, primary_key=True)
    analyzed_count = Column(Integer, nullable=False)
    date_range_start = Column(DateTime(timezone=True), nullable=False)
    date_range_end = Column(DateTime(timezone=True), nullable=False)
    video_styles = Column(JSON, nullable=False)  # List of {category, confidence, count}
    common_patterns = Column(JSON, nullable=False)  # List of pattern objects
    avg_engagement_velocity = Column(Float)
    top_hashtags = Column(JSON)  # List of hashtag strings
    recommendations = Column(JSON)  # List of recommendation strings
    raw_report = Column(JSON)  # Full Claude response for debugging
    created_at = Column(DateTime(timezone=True), server_default=func.now())
```

**2. Create Alembic migration `alembic/versions/002_trend_intelligence_schema.py`:**

Revision ID: '002', down_revision: '001'.

In upgrade():
- Add columns to trends table: description (Text), duration (Integer), creator_id (String(255)), sound_name (String(500)), posted_at (DateTime), engagement_velocity (Float)
- Drop the existing unique constraint on external_id (use `op.drop_constraint` -- for SQLite, use batch operations: `with op.batch_alter_table('trends') as batch_op:`)
- Add composite unique constraint on (platform, external_id) named 'uq_platform_external_id' (also via batch_alter_table for SQLite)
- Create trend_reports table with all columns listed above

IMPORTANT: SQLite requires batch_alter_table for constraint changes. Use this pattern:
```python
with op.batch_alter_table('trends') as batch_op:
    batch_op.add_column(sa.Column('description', sa.Text(), nullable=True))
    batch_op.add_column(sa.Column('duration', sa.Integer(), nullable=True))
    batch_op.add_column(sa.Column('creator_id', sa.String(255), nullable=True))
    batch_op.add_column(sa.Column('sound_name', sa.String(500), nullable=True))
    batch_op.add_column(sa.Column('posted_at', sa.DateTime(timezone=True), nullable=True))
    batch_op.add_column(sa.Column('engagement_velocity', sa.Float(), nullable=True))
    batch_op.drop_constraint('uq_trends_external_id', type_='unique')  # or whatever the existing constraint name is
    batch_op.create_unique_constraint('uq_platform_external_id', ['platform', 'external_id'])
```

Use `sa.text('CURRENT_TIMESTAMP')` for server_default (not `func.now()` -- this is a migration file, not a model). Python 3.9 compatible -- use `from typing import Sequence, Union`.

In downgrade(): Drop trend_reports table, reverse column additions with batch_alter_table.

**3. Update app/config.py:**

Add these fields to the Settings class:
```python
# Trend Intelligence
use_mock_data: bool = True  # Default to mock for local dev
apify_api_token: str = ""
youtube_api_key: str = ""
anthropic_api_key: str = ""

# Schedule
trend_scrape_interval_hours: int = 6
trend_analysis_delay_minutes: int = 30
```

**4. Update .env:**

Add these lines (empty defaults for API keys):
```
# Trend Intelligence
USE_MOCK_DATA=true
APIFY_API_TOKEN=
YOUTUBE_API_KEY=
ANTHROPIC_API_KEY=
```

**5. Update requirements.txt:**

Add these dependencies (append, do not remove existing):
```
anthropic
httpx
tenacity
google-api-python-client
google-auth-oauthlib
isodate
pytest
pytest-mock
pytest-asyncio
```

Do NOT add `apify-client` yet -- we will use httpx to call Apify REST API directly (simpler, fewer deps). Do NOT add `celery-pool-asyncio` -- we will use `asyncio.run()` pattern in Celery tasks (simpler, proven).
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "from app.models import Trend, TrendReport; print('Models OK'); print([c.name for c in Trend.__table__.columns]); print([c.name for c in TrendReport.__table__.columns])"`

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "from app.config import get_settings; s = get_settings(); print(f'mock={s.use_mock_data}, apify={s.apify_api_token!r}')"`

Run: `cd /Users/naokitsk/Documents/short-video-generator && alembic upgrade head` -- migration runs without error.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "import sqlite3; conn = sqlite3.connect('viralforge.db'); cursor = conn.execute('PRAGMA table_info(trends)'); cols = [row[1] for row in cursor.fetchall()]; print(cols); assert 'description' in cols; assert 'duration' in cols; assert 'posted_at' in cols; assert 'engagement_velocity' in cols; print('Schema OK')"` -- verify columns exist.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "import sqlite3; conn = sqlite3.connect('viralforge.db'); cursor = conn.execute('PRAGMA table_info(trend_reports)'); cols = [row[1] for row in cursor.fetchall()]; print(cols); assert 'video_styles' in cols; print('TrendReport table OK')"`
  </verify>
  <done>
Trend model has all TREND-03 metadata fields. TrendReport table exists with structured analysis columns. Composite unique constraint on (platform, external_id) enforced. Config has use_mock_data=True default and API key settings. Migration runs cleanly on SQLite.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Pydantic schemas and mock data fixtures</name>
  <files>
    app/schemas.py
    app/scrapers/__init__.py
    app/scrapers/mock_data/tiktok_trending.json
    app/scrapers/mock_data/youtube_shorts.json
  </files>
  <action>
**1. Create app/schemas.py:**

Define Pydantic v2 models for data validation and transfer. All must be Python 3.9 compatible (use `from typing import List, Optional` not `list[str]`).

```python
from pydantic import BaseModel, Field
from typing import List, Optional
from datetime import datetime


class TrendCreate(BaseModel):
    """Schema for creating/updating a trend from scraper output."""
    platform: str  # "tiktok" or "youtube"
    external_id: str
    title: Optional[str] = None
    description: Optional[str] = None
    hashtags: Optional[List[str]] = None
    views: Optional[int] = 0
    likes: Optional[int] = 0
    comments: Optional[int] = 0
    shares: Optional[int] = 0
    duration: Optional[int] = None  # seconds
    creator: Optional[str] = None
    creator_id: Optional[str] = None
    sound_name: Optional[str] = None
    video_url: Optional[str] = None
    thumbnail_url: Optional[str] = None
    posted_at: Optional[datetime] = None


class TrendResponse(BaseModel):
    """Schema for trend API responses."""
    id: int
    platform: str
    external_id: str
    title: Optional[str]
    description: Optional[str]
    hashtags: Optional[List[str]]
    views: Optional[int]
    likes: Optional[int]
    comments: Optional[int]
    shares: Optional[int]
    duration: Optional[int]
    creator: Optional[str]
    engagement_velocity: Optional[float]
    collected_at: Optional[datetime]

    class Config:
        from_attributes = True


class VideoStyleSchema(BaseModel):
    """Style classification for a group of videos."""
    category: str  # cinematic, talking-head, montage, text-heavy, animation
    confidence: float
    count: int


class TrendPatternSchema(BaseModel):
    """Extracted pattern from trending videos."""
    format_description: str
    avg_duration_seconds: float
    hook_type: str  # question, shock, story, tutorial
    uses_text_overlay: bool
    audio_type: str  # original, trending-sound, voiceover, music


class TrendReportCreate(BaseModel):
    """Schema for Claude analysis output."""
    analyzed_count: int
    video_styles: List[VideoStyleSchema]
    common_patterns: List[TrendPatternSchema]
    avg_engagement_velocity: float
    top_hashtags: List[str]
    recommendations: List[str]


class TrendReportResponse(BaseModel):
    """Schema for trend report API responses."""
    id: int
    analyzed_count: int
    date_range_start: datetime
    date_range_end: datetime
    video_styles: List[VideoStyleSchema]
    common_patterns: List[TrendPatternSchema]
    avg_engagement_velocity: Optional[float]
    top_hashtags: Optional[List[str]]
    recommendations: Optional[List[str]]
    created_at: Optional[datetime]

    class Config:
        from_attributes = True
```

**2. Create app/scrapers/__init__.py:**
Empty file (package marker).

**3. Create app/scrapers/mock_data/ directory and tiktok_trending.json:**

Create a JSON array with 10 realistic TikTok trending video entries. Each entry MUST have ALL these fields to match TrendCreate schema:
- external_id (string, TikTok video ID format like "7340123456789012345")
- title, description (realistic short-form video titles/descriptions)
- hashtags (array of 3-5 relevant strings)
- views, likes, comments, shares (realistic numbers -- viral range: 500K-10M views)
- duration (15-60 seconds)
- creator (username string), creator_id (numeric string)
- sound_name (realistic TikTok sound names)
- video_url (format: "https://www.tiktok.com/@creator/video/{external_id}")
- thumbnail_url (format: "https://p16-sign-sg.tiktokcdn.com/tos-alisg-p-0068/{id}~tplv-tiktokx-zoom-crop-mark.jpg")
- posted_at (ISO 8601 timestamps within last 48 hours from a reference date of "2026-02-13T12:00:00Z" -- make them spread across the 48h window)

Make the data diverse: different creators, different hashtag themes (fitness, cooking, comedy, tech, dance, pets, DIY, travel, fashion, gaming). Vary engagement numbers realistically.

**4. Create app/scrapers/mock_data/youtube_shorts.json:**

Same structure but for YouTube Shorts:
- external_id (YouTube video ID format like "dQw4w9WgXcQ" -- 11 chars)
- title, description (YouTube-style titles)
- hashtags (array, from YouTube tags)
- views, likes, comments (realistic), shares: 0 (YouTube API doesn't expose shares)
- duration (15-58 seconds -- all under 60 to be Shorts)
- creator (channel name), creator_id ("UC" + alphanumeric)
- sound_name: "" (YouTube doesn't have sound attribution like TikTok)
- video_url ("https://www.youtube.com/shorts/{external_id}")
- thumbnail_url ("https://i.ytimg.com/vi/{external_id}/hqdefault.jpg")
- posted_at (ISO 8601, within last 48 hours from reference)

10 entries, diverse topics.
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "from app.schemas import TrendCreate, TrendReportCreate, VideoStyleSchema, TrendPatternSchema; print('All schemas import OK')"` -- no import errors.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
import json
from app.schemas import TrendCreate

with open('app/scrapers/mock_data/tiktok_trending.json') as f:
    data = json.load(f)
print(f'TikTok: {len(data)} entries')
for item in data:
    t = TrendCreate(platform='tiktok', **item)
    assert t.external_id
    assert t.duration and t.duration > 0
    assert t.posted_at is not None
print('All TikTok mock data validates')

with open('app/scrapers/mock_data/youtube_shorts.json') as f:
    data = json.load(f)
print(f'YouTube: {len(data)} entries')
for item in data:
    t = TrendCreate(platform='youtube', **item)
    assert t.external_id
    assert t.duration and t.duration < 60
print('All YouTube mock data validates')
"` -- all mock data entries validate against Pydantic schemas.
  </verify>
  <done>
Pydantic schemas validate all trend and report data. Mock data fixtures contain 10 realistic entries each for TikTok and YouTube, all passing schema validation. Mock data covers diverse content categories with realistic engagement numbers.
  </done>
</task>

</tasks>

<verification>
1. `alembic upgrade head` runs without errors
2. All new model columns exist in SQLite database
3. Pydantic schemas import and validate correctly
4. Mock data files parse and validate against schemas
5. Config returns use_mock_data=True by default
6. Python 3.9 compatibility (no 3.10+ syntax anywhere)
</verification>

<success_criteria>
- Trend model has all 6 new columns (description, duration, creator_id, sound_name, posted_at, engagement_velocity)
- Composite unique constraint on (platform, external_id) exists
- TrendReport table exists with video_styles, common_patterns, recommendations columns
- TrendCreate, TrendReportCreate, TrendReportResponse schemas validate correctly
- 10 TikTok + 10 YouTube mock data entries exist and validate
- USE_MOCK_DATA defaults to true in config
- requirements.txt includes all new dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/02-trend-intelligence/02-01-SUMMARY.md`
</output>
