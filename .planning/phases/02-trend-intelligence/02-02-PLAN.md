---
phase: 02-trend-intelligence
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - app/scrapers/tiktok.py
  - app/scrapers/youtube.py
  - app/scrapers/base.py
  - app/services/__init__.py
  - app/services/trend_collector.py
  - app/services/engagement.py
  - app/tasks.py
autonomous: true

must_haves:
  truths:
    - "TikTok scraper returns 50 normalized trend items in mock mode"
    - "YouTube scraper returns 50 normalized trend items in mock mode"
    - "Scrapers switch between mock and real API via USE_MOCK_DATA config"
    - "Trends saved to database with UPSERT deduplication on (platform, external_id)"
    - "Engagement velocity calculated as (likes+comments+shares)/hours_since_posted"
    - "Celery task collects from both platforms and saves to DB in one run"
  artifacts:
    - path: "app/scrapers/tiktok.py"
      provides: "TikTok scraper with Apify integration and mock mode"
      contains: "def scrape_tiktok"
    - path: "app/scrapers/youtube.py"
      provides: "YouTube scraper with Data API v3 and mock mode"
      contains: "def scrape_youtube_shorts"
    - path: "app/services/engagement.py"
      provides: "Engagement velocity calculator"
      contains: "def calculate_engagement_velocity"
    - path: "app/services/trend_collector.py"
      provides: "DB save with UPSERT + orchestration"
      contains: "async def save_trends"
    - path: "app/tasks.py"
      provides: "Celery task for trend collection"
      contains: "def collect_trends_task"
  key_links:
    - from: "app/scrapers/tiktok.py"
      to: "app/scrapers/mock_data/tiktok_trending.json"
      via: "loads fixture when USE_MOCK_DATA=true"
      pattern: "tiktok_trending\\.json"
    - from: "app/scrapers/youtube.py"
      to: "app/scrapers/mock_data/youtube_shorts.json"
      via: "loads fixture when USE_MOCK_DATA=true"
      pattern: "youtube_shorts\\.json"
    - from: "app/services/trend_collector.py"
      to: "app/models.py"
      via: "SQLAlchemy ORM UPSERT"
      pattern: "from app\\.models import Trend"
    - from: "app/tasks.py"
      to: "app/services/trend_collector.py"
      via: "Celery task calls collector"
      pattern: "collect_trends"
---

<objective>
Build the trend collection pipeline: TikTok and YouTube scrapers with mock/real switching, engagement velocity calculation, database UPSERT operations, and a Celery task that orchestrates the full collection run.

Purpose: This is the "collect" half of trend intelligence -- getting raw data into the database so the analysis plan can consume it.
Output: Working scrapers, DB save operations, Celery collection task -- all testable with mock data.
</objective>

<execution_context>
@/Users/naokitsk/.claude/get-shit-done/workflows/execute-plan.md
@/Users/naokitsk/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-trend-intelligence/02-RESEARCH.md
@.planning/phases/02-trend-intelligence/02-01-SUMMARY.md

@app/models.py
@app/schemas.py
@app/config.py
@app/database.py
@app/worker.py
@app/tasks.py
@app/scrapers/mock_data/tiktok_trending.json
@app/scrapers/mock_data/youtube_shorts.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: TikTok and YouTube scrapers with mock/real switching</name>
  <files>
    app/scrapers/base.py
    app/scrapers/tiktok.py
    app/scrapers/youtube.py
  </files>
  <action>
**1. Create app/scrapers/base.py -- shared scraper utilities:**

```python
import json
import logging
from pathlib import Path
from typing import List, Dict, Any

logger = logging.getLogger(__name__)

MOCK_DATA_DIR = Path(__file__).parent / "mock_data"


def load_mock_data(filename: str) -> List[Dict[str, Any]]:
    """Load mock data from JSON fixture file."""
    filepath = MOCK_DATA_DIR / filename
    with open(filepath, "r") as f:
        data = json.load(f)
    logger.info(f"Loaded {len(data)} items from mock data: {filename}")
    return data
```

**2. Create app/scrapers/tiktok.py:**

Implement `scrape_tiktok_trends(limit: int = 50) -> List[Dict[str, Any]]` function.

Mock mode (when `settings.use_mock_data` is True):
- Load from `mock_data/tiktok_trending.json` using `load_mock_data()`
- Duplicate/cycle entries to reach `limit` count (if fixture has 10 items and limit is 50, cycle through with modified external_ids like appending "_dup_1", "_dup_2" etc.)
- Return list of dicts matching TrendCreate schema field names

Real mode (when `settings.use_mock_data` is False):
- Use `httpx` to call Apify REST API (NOT apify-client library):
  - POST to `https://api.apify.com/v2/acts/lexis-solutions~tiktok-trending-videos-scraper/runs` with headers `{"Authorization": f"Bearer {settings.apify_api_token}"}` and JSON body `{"maxResults": limit}`
  - Poll the run status at `https://api.apify.com/v2/actor-runs/{run_id}` until SUCCEEDED (with 10s sleep between polls, max 5 minutes)
  - Fetch results from `https://api.apify.com/v2/datasets/{dataset_id}/items`
- Wrap real API calls with `tenacity.retry` decorator: `stop_after_attempt(3)`, `wait_exponential(multiplier=2, min=4, max=60)`, retry on `httpx.HTTPStatusError` and `httpx.ConnectError`
- Normalize Apify response to internal format (map fields to TrendCreate field names)
- Log each step (starting scrape, got N results, etc.)

IMPORTANT: This is a synchronous function (not async). Celery tasks call it synchronously. httpx can be used in sync mode with `httpx.Client()`.

Use `from app.config import get_settings` to get settings.

**3. Create app/scrapers/youtube.py:**

Implement `scrape_youtube_shorts(limit: int = 50) -> List[Dict[str, Any]]` function.

Mock mode: Same pattern as TikTok -- load from youtube_shorts.json, cycle to reach limit.

Real mode:
- Use `googleapiclient.discovery.build('youtube', 'v3', developerKey=settings.youtube_api_key)`
- Step 1: `youtube.search().list(part='id', type='video', videoDuration='short', order='viewCount', publishedAfter=(7 days ago ISO), maxResults=min(limit, 50)).execute()` -- costs 100 quota units
- Step 2: Extract video IDs, batch fetch details: `youtube.videos().list(part='snippet,statistics,contentDetails', id=','.join(video_ids), fields='items(id,snippet(title,description,tags,thumbnails,channelTitle,channelId,publishedAt),statistics(viewCount,likeCount,commentCount),contentDetails(duration))').execute()` -- costs 1 unit
- Step 3: Parse duration with `isodate.parse_duration()`, filter to < 60 seconds (actual Shorts)
- Step 4: Normalize to internal format. Map: channelTitle->creator, channelId->creator_id, tags->hashtags, etc. shares=0 (YouTube doesn't expose). sound_name="" (not available).
- Wrap with tenacity retry (same config as TikTok)
- If filtered results < limit, log a warning but return what we have

IMPORTANT: Synchronous function. Use regular `googleapiclient` (it's synchronous).

Both scrapers must handle exceptions gracefully -- catch and log errors, return empty list on total failure (don't crash the Celery task).
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
from app.scrapers.tiktok import scrape_tiktok_trends
from app.scrapers.youtube import scrape_youtube_shorts

# Test mock mode (USE_MOCK_DATA=true by default)
tt = scrape_tiktok_trends(limit=5)
print(f'TikTok: {len(tt)} trends')
assert len(tt) == 5
assert all('external_id' in t for t in tt)
assert all('title' in t for t in tt)
assert all('posted_at' in t for t in tt)

yt = scrape_youtube_shorts(limit=5)
print(f'YouTube: {len(yt)} trends')
assert len(yt) == 5
assert all('external_id' in t for t in yt)
assert all(t.get('duration', 0) < 60 or t.get('duration') is None for t in yt)
print('Both scrapers work in mock mode')
"`
  </verify>
  <done>
Both scrapers return normalized trend dicts in mock mode. Real API integration code exists but only activates when USE_MOCK_DATA=false. Retry logic configured with tenacity for real API calls.
  </done>
</task>

<task type="auto">
  <name>Task 2: Engagement velocity calculator and database UPSERT operations</name>
  <files>
    app/services/__init__.py
    app/services/engagement.py
    app/services/trend_collector.py
  </files>
  <action>
**1. Create app/services/__init__.py:**
Empty file (package marker).

**2. Create app/services/engagement.py:**

Implement `calculate_engagement_velocity(trend_dict: Dict) -> float`:
- Formula: `(likes + comments + shares) / hours_since_posted`
- Parse `posted_at` from ISO string: `datetime.fromisoformat(posted_at.replace('Z', '+00:00'))`
- Calculate hours since posted: `(datetime.now(timezone.utc) - posted_at).total_seconds() / 3600`
- Minimum hours threshold: 0.1 (6 minutes) to prevent division by zero
- Return float rounded to 2 decimal places
- Handle missing fields gracefully: default likes/comments/shares to 0, return 0.0 if posted_at is None

Also implement `enrich_trends_with_velocity(trends: List[Dict]) -> List[Dict]`:
- Loop through trends, calculate velocity for each
- Add `engagement_velocity` key to each dict
- Sort by engagement_velocity descending
- Return enriched list

Use `from typing import List, Dict` for Python 3.9 compat.

**3. Create app/services/trend_collector.py:**

This is the core orchestration service. Implements:

`async def save_trends(trends: List[Dict[str, Any]], platform: str) -> int`:
- Takes list of trend dicts (from scrapers) and platform name
- Uses SQLAlchemy async session from `app.database.async_session_factory`
- For each trend dict:
  - Create a TrendCreate pydantic model to validate
  - Use SQLAlchemy `insert().on_conflict_do_update()` for UPSERT on (platform, external_id)
  - On conflict, UPDATE: likes, comments, shares, views, engagement_velocity, collected_at
  - IMPORTANT: For SQLite, use `from sqlalchemy.dialects.sqlite import insert as sqlite_insert` -- the `on_conflict_do_update` is dialect-specific
- Commit after all inserts
- Return count of saved/updated trends
- Log results: "Saved {N} {platform} trends"

`async def collect_all_trends() -> Dict[str, int]`:
- Import scrapers: `from app.scrapers.tiktok import scrape_tiktok_trends` and youtube equivalent
- Import engagement: `from app.services.engagement import enrich_trends_with_velocity`
- Call both scrapers (synchronous calls)
- Enrich each platform's results with engagement velocity
- Save each platform's trends to DB
- Return dict like `{"tiktok": 50, "youtube": 45}` with counts

Handle the fact that scrapers are sync but save_trends is async. The `collect_all_trends` function is async (calls save_trends which uses async session). The Celery task will wrap it with `asyncio.run()`.

Error handling: If one platform's scraper fails, log the error and continue with the other. Don't let TikTok failure prevent YouTube collection.
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
from app.services.engagement import calculate_engagement_velocity, enrich_trends_with_velocity
from datetime import datetime, timezone, timedelta

# Test velocity calculation
trend = {
    'likes': 1000,
    'comments': 200,
    'shares': 50,
    'posted_at': (datetime.now(timezone.utc) - timedelta(hours=5)).isoformat()
}
v = calculate_engagement_velocity(trend)
expected = (1000 + 200 + 50) / 5.0
print(f'Velocity: {v} (expected ~{expected:.2f})')
assert abs(v - expected) < 1.0  # Allow small time drift
print('Velocity calculation OK')

# Test enrichment
trends = [trend, {'likes': 500, 'comments': 100, 'shares': 25, 'posted_at': None}]
enriched = enrich_trends_with_velocity(trends)
assert enriched[0]['engagement_velocity'] > 0
assert enriched[1]['engagement_velocity'] == 0.0
print('Enrichment OK')
"` -- velocity calculation works.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
import asyncio
from app.services.trend_collector import collect_all_trends

result = asyncio.run(collect_all_trends())
print(f'Collection result: {result}')
assert 'tiktok' in result
assert 'youtube' in result
assert result['tiktok'] > 0
assert result['youtube'] > 0
print('Full collection pipeline works with mock data')
"` -- end-to-end collection from mock data to DB.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
import asyncio
from app.services.trend_collector import collect_all_trends

# Run twice to test UPSERT deduplication
asyncio.run(collect_all_trends())
result2 = asyncio.run(collect_all_trends())
print(f'Second run: {result2}')

# Check no duplicates in DB
import sqlite3
conn = sqlite3.connect('viralforge.db')
cursor = conn.execute('SELECT COUNT(*) FROM trends')
count = cursor.fetchone()[0]
print(f'Total trends in DB: {count}')
# Should be ~20 (10 tiktok + 10 youtube mock unique), not ~40
assert count <= 25, f'Too many trends ({count}), UPSERT dedup may be broken'
print('UPSERT deduplication working')
conn.close()
"` -- verify deduplication.
  </verify>
  <done>
Engagement velocity formula implemented with division-by-zero protection. UPSERT saves trends with deduplication on (platform, external_id). collect_all_trends() orchestrates both platforms and stores enriched results. Running twice produces no duplicates.
  </done>
</task>

<task type="auto">
  <name>Task 3: Celery collection task and API endpoint</name>
  <files>
    app/tasks.py
    app/api/routes.py
  </files>
  <action>
**1. Update app/tasks.py:**

Keep the existing `test_task`. Add a new task:

```python
import asyncio
import logging

logger = logging.getLogger(__name__)

@celery_app.task(
    bind=True,
    name='app.tasks.collect_trends_task',
    max_retries=3,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=600,
    retry_jitter=True,
)
def collect_trends_task(self):
    """Collect trending videos from TikTok and YouTube."""
    logger.info(f"Starting trend collection (attempt {self.request.retries + 1})")
    try:
        from app.services.trend_collector import collect_all_trends
        result = asyncio.run(collect_all_trends())
        logger.info(f"Trend collection complete: {result}")
        return {"status": "success", "collected": result}
    except Exception as exc:
        logger.error(f"Trend collection failed: {exc}")
        raise
```

**2. Update app/api/routes.py:**

Add these endpoints (keep existing /health and /test-task):

```python
@router.post("/collect-trends")
async def trigger_trend_collection():
    """Trigger trend collection from TikTok and YouTube."""
    from app.tasks import collect_trends_task
    task = collect_trends_task.delay()
    return {"task_id": str(task.id), "status": "queued", "description": "Collecting trends from TikTok and YouTube"}

@router.get("/trends")
async def list_trends(
    platform: str = None,
    limit: int = 50,
    session: AsyncSession = Depends(get_session)
):
    """List collected trends, optionally filtered by platform."""
    from sqlalchemy import select
    from app.models import Trend

    query = select(Trend).order_by(Trend.collected_at.desc()).limit(limit)
    if platform:
        query = query.where(Trend.platform == platform)

    result = await session.execute(query)
    trends = result.scalars().all()

    return {
        "count": len(trends),
        "trends": [
            {
                "id": t.id,
                "platform": t.platform,
                "external_id": t.external_id,
                "title": t.title,
                "creator": t.creator,
                "likes": t.likes,
                "comments": t.comments,
                "shares": t.shares,
                "views": t.views,
                "duration": t.duration,
                "engagement_velocity": t.engagement_velocity,
                "collected_at": t.collected_at.isoformat() if t.collected_at else None,
            }
            for t in trends
        ]
    }
```

Add `Optional` import from typing if not already present. Add `str = None` default for platform parameter (Python 3.9 compatible, not `str | None`).
  </action>
  <verify>
Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "from app.tasks import collect_trends_task; print(f'Task name: {collect_trends_task.name}'); print('Task registered OK')"` -- task imports without error.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

# First, collect some trends directly (not via Celery)
import asyncio
from app.services.trend_collector import collect_all_trends
asyncio.run(collect_all_trends())

# Test /trends endpoint
response = client.get('/trends?limit=5')
print(f'GET /trends: {response.status_code}')
assert response.status_code == 200
data = response.json()
print(f'Count: {data[\"count\"]}')
assert data['count'] > 0
assert 'engagement_velocity' in data['trends'][0]
print('Trends endpoint working')

# Test platform filter
response = client.get('/trends?platform=tiktok&limit=5')
assert response.status_code == 200
data = response.json()
assert all(t['platform'] == 'tiktok' for t in data['trends'])
print('Platform filter working')
"` -- API endpoint returns collected trends.

Run: `cd /Users/naokitsk/Documents/short-video-generator && python -c "
from fastapi.testclient import TestClient
from app.main import app
client = TestClient(app)
response = client.post('/collect-trends')
print(f'POST /collect-trends: {response.status_code}')
assert response.status_code == 200
data = response.json()
assert 'task_id' in data
print(f'Task queued: {data}')
"` -- collection trigger endpoint works.
  </verify>
  <done>
Celery collect_trends_task runs both scrapers and saves to DB with retry logic. POST /collect-trends queues collection. GET /trends returns stored trends with optional platform filter. All working with mock data.
  </done>
</task>

</tasks>

<verification>
1. Both scrapers return normalized trend data in mock mode
2. Engagement velocity calculated correctly with edge case protection
3. UPSERT prevents duplicates when running collection multiple times
4. Celery task wraps async collection with asyncio.run()
5. API endpoints return collected trends
6. All code Python 3.9 compatible
</verification>

<success_criteria>
- `scrape_tiktok_trends(limit=50)` returns 50 normalized trend dicts in mock mode
- `scrape_youtube_shorts(limit=50)` returns 50 normalized trend dicts in mock mode
- `collect_all_trends()` saves to DB and returns counts for both platforms
- Running collection twice does NOT create duplicate rows (UPSERT works)
- `GET /trends` returns stored trends with engagement_velocity field
- `POST /collect-trends` queues a Celery task
- All engagement velocities are finite positive numbers (no inf, no NaN)
</success_criteria>

<output>
After completion, create `.planning/phases/02-trend-intelligence/02-02-SUMMARY.md`
</output>
